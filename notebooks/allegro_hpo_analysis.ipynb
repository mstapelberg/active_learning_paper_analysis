{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : I need to compile and test all of the models on an identical testset, run a cross-validation on the top 10 models from an accuracy standpoint, like I did for MACE. \n",
    "\n",
    "Currently, I only have the validation results for the models, not the test results, I should modify the evaluate_config.py to also accept an allegro model in addition to a MACE model. \n",
    "\n",
    "Additionally, I need to refactor FORGE to do db to MLIP better, get the config_type working in the database, and fully use the forge workflow to deploy generation 7 models (instead of using my manual deployment process). Also need to see why the energy, force, and stress get saved in the metadata section as well as the calculation table \n",
    "\n",
    "1. get all of the OUTCARs from gen 7\n",
    "2. get them into an xyz file \n",
    "3. add them to the database with the correct metadata\n",
    "4. deploy MACE MLIP training script on PSFC-GPU cluster with pair-repulsion added \n",
    "5. train the 5 models in the ensemble \n",
    "6. use generation 7 models to do Adversarial Attack for generation 8 model --> make 2000 more datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'State', 'Notes', 'User', 'Tags', 'Created', 'Runtime', 'Sweep',\n",
      "       'AllegroBesselBasis_trainable', 'PolynomialCutoff_p',\n",
      "       ...\n",
      "       'validation_e_mae', 'validation_f_mae', 'validation_f_rmse',\n",
      "       'validation_loss', 'validation_loss_e', 'validation_loss_f',\n",
      "       'validation_loss_stress', 'validation_stress_mae',\n",
      "       'validation_stress_rmse', 'wall'],\n",
      "      dtype='object', length=109)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/hpo_analysis/allegro_wandb_export_2025-03-18T23_09_11.553-04_00.csv')\n",
    "\n",
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allegro_model_161_r_max8p0_l_max2_num_layers2_num_tensor_features64_learning_rate0p001\n"
     ]
    }
   ],
   "source": [
    "print(df['Name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: Index(['Name', 'test0_epoch/forces_mae', 'test0_epoch/per_atom_energy_mae',\n",
      "       'test0_epoch/total_energy_mae', 'test0_epoch/stress_mae',\n",
      "       'val0_epoch/total_energy_mae', 'val0_epoch/stress_mae',\n",
      "       'val0_epoch/per_atom_energy_mae', 'val0_epoch/forces_mae', 'epoch'],\n",
      "      dtype='object')\n",
      "\n",
      "DataFrame after parsing 'Name' column (first 5 rows):\n",
      "                                                Name  test0_epoch/forces_mae  \\\n",
      "0  hpo_107_num_layers-2_l_max-2_num_scalar_featur...                0.174025   \n",
      "1  hpo_107_num_layers-2_l_max-2_num_scalar_featur...                0.180055   \n",
      "2  hpo_107_num_layers-2_l_max-2_num_scalar_featur...                0.169609   \n",
      "3  hpo_107_num_layers-2_l_max-2_num_scalar_featur...                0.168025   \n",
      "4  hpo_107_num_layers-2_l_max-2_num_scalar_featur...                0.170709   \n",
      "\n",
      "   test0_epoch/per_atom_energy_mae  test0_epoch/total_energy_mae  \\\n",
      "0                         0.022693                      2.136762   \n",
      "1                         0.023742                      2.245516   \n",
      "2                         0.021612                      2.158613   \n",
      "3                         0.020831                      2.011517   \n",
      "4                         0.022079                      2.119971   \n",
      "\n",
      "   test0_epoch/stress_mae  val0_epoch/total_energy_mae  val0_epoch/stress_mae  \\\n",
      "0                0.007491                     2.177285               0.007363   \n",
      "1                0.007328                     2.265586               0.007255   \n",
      "2                0.006815                     2.200782               0.006829   \n",
      "3                0.006951                     2.020366               0.006795   \n",
      "4                0.007379                     2.109730               0.007142   \n",
      "\n",
      "   val0_epoch/per_atom_energy_mae  val0_epoch/forces_mae  epoch  hpo_run_id  \\\n",
      "0                        0.023601               0.178880    100         107   \n",
      "1                        0.023849               0.184744    100         107   \n",
      "2                        0.022604               0.174700    100         107   \n",
      "3                        0.021421               0.178075    100         107   \n",
      "4                        0.022443               0.180827    100         107   \n",
      "\n",
      "   num_layers  l_max  num_scalar_features  num_tensor_features  mlp_width  \\\n",
      "0           2      2                  128                   64        512   \n",
      "1           2      2                  128                   64        512   \n",
      "2           2      2                  128                   64        512   \n",
      "3           2      2                  128                   64        512   \n",
      "4           2      2                  128                   64        512   \n",
      "\n",
      "   fold  seed  hpo_config_signature  \n",
      "0     2     2  (2, 2, 128, 64, 512)  \n",
      "1     2     1  (2, 2, 128, 64, 512)  \n",
      "2     2     0  (2, 2, 128, 64, 512)  \n",
      "3     1     2  (2, 2, 128, 64, 512)  \n",
      "4     1     1  (2, 2, 128, 64, 512)  \n",
      "\n",
      "Test metric columns: []\n",
      "\n",
      "Found 108 unique HPO configurations after cleaning.\n",
      "\n",
      "Number of runs per HPO configuration (top 10):\n",
      "hpo_config_signature\n",
      "(1, 1, 32, 16, 128)    9\n",
      "(1, 1, 32, 16, 256)    9\n",
      "(1, 1, 32, 16, 512)    9\n",
      "(1, 1, 32, 32, 128)    9\n",
      "(1, 1, 32, 32, 256)    9\n",
      "(1, 1, 32, 32, 512)    9\n",
      "(1, 1, 32, 64, 128)    9\n",
      "(1, 1, 32, 64, 256)    9\n",
      "(1, 1, 32, 64, 512)    9\n",
      "(1, 1, 64, 16, 128)    9\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Analysis Complete.\n",
      "Post-hoc power is descriptive. Effect sizes (Cohen's d) provide context to p-values.\n",
      "\n",
      "Skipping example of specific pairwise comparison due to lack of processed metrics or configurations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/hpo_analysis/new_allegro/wandb_export_2025-05-17T11_11_30.422-04_00.csv')\n",
    "\n",
    "print(f\"Original columns: {df.columns}\")\n",
    "# print(\"Original DataFrame head:\")\n",
    "# print(df.head())\n",
    "\n",
    "# Function to parse the 'Name' column\n",
    "def parse_name(name_str):\n",
    "    params = {}\n",
    "    match = re.search(r'hpo_(\\d+)', name_str)\n",
    "    if match:\n",
    "        params['hpo_run_id'] = int(match.group(1))\n",
    "    else:\n",
    "        params['hpo_run_id'] = None\n",
    "\n",
    "    core_hyperparameters = [\n",
    "        \"num_layers\", \"l_max\", \"num_scalar_features\", \n",
    "        \"num_tensor_features\", \"mlp_width\"\n",
    "    ]\n",
    "    \n",
    "    for key in core_hyperparameters:\n",
    "        match = re.search(rf'{key}-([\\d\\.]+)', name_str)\n",
    "        if match:\n",
    "            val_str = match.group(1)\n",
    "            try:\n",
    "                params[key] = int(val_str)\n",
    "            except ValueError:\n",
    "                params[key] = float(val_str)\n",
    "        else:\n",
    "            params[key] = np.nan\n",
    "\n",
    "    match = re.search(r'fold(\\d+)', name_str)\n",
    "    if match:\n",
    "        params['fold'] = int(match.group(1))\n",
    "    else:\n",
    "        params['fold'] = np.nan\n",
    "\n",
    "    match = re.search(r'seed(\\d+)', name_str)\n",
    "    if match:\n",
    "        params['seed'] = int(match.group(1))\n",
    "    else:\n",
    "        params['seed'] = np.nan\n",
    "        \n",
    "    config_values = [params.get(key) for key in core_hyperparameters]\n",
    "    params['hpo_config_signature'] = tuple(config_values)\n",
    "        \n",
    "    return pd.Series(params)\n",
    "\n",
    "parsed_df = df['Name'].apply(parse_name)\n",
    "df = pd.concat([df, parsed_df], axis=1)\n",
    "\n",
    "print(\"\\nDataFrame after parsing 'Name' column (first 5 rows):\")\n",
    "print(df.head())\n",
    "\n",
    "core_hpo_params_for_cleaning = [\"num_layers\", \"l_max\", \"num_scalar_features\", \"num_tensor_features\", \"mlp_width\"]\n",
    "df.dropna(subset=['hpo_config_signature'] + core_hpo_params_for_cleaning, how='any', inplace=True)\n",
    "\n",
    "for col in core_hpo_params_for_cleaning:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df.dropna(subset=core_hpo_params_for_cleaning, inplace=True)\n",
    "\n",
    "test_metric_columns = [col for col in df.columns if col.startswith('test_') and pd.api.types.is_numeric_dtype(df[col])]\n",
    "print(f\"\\nTest metric columns: {test_metric_columns}\")\n",
    "\n",
    "unique_configs = df['hpo_config_signature'].unique()\n",
    "print(f\"\\nFound {len(unique_configs)} unique HPO configurations after cleaning.\")\n",
    "print(\"\\nNumber of runs per HPO configuration (top 10):\")\n",
    "print(df.groupby('hpo_config_signature').size().sort_values(ascending=False).head(10))\n",
    "\n",
    "# --- Helper function for pairwise statistical comparison ---\n",
    "def perform_pairwise_comparison(data1, data2, alpha_corrected):\n",
    "    \"\"\"\n",
    "    Performs Mann-Whitney U test, calculates Cohen's d, and post-hoc power\n",
    "    for two independent groups.\n",
    "    \n",
    "    Args:\n",
    "        data1 (array-like): Data for group 1.\n",
    "        data2 (array-like): Data for group 2.\n",
    "        alpha_corrected (float): Corrected significance level for the test.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains p_value, cohen_d, power, and significance status.\n",
    "    \"\"\"\n",
    "    if len(data1) < 2 or len(data2) < 2:\n",
    "        return {\n",
    "            'p_value': np.nan, 'cohen_d': np.nan, 'power': np.nan,\n",
    "            'significant': False, 'error': 'Insufficient data'\n",
    "        }\n",
    "\n",
    "    # Mann-Whitney U test\n",
    "    try:\n",
    "        u_stat, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "    except ValueError as e: # Handles cases like all values being identical\n",
    "         return {\n",
    "            'p_value': 1.0 if np.array_equal(data1, data2) else np.nan, # Or handle as appropriate\n",
    "            'cohen_d': 0.0 if np.array_equal(data1, data2) else np.nan,\n",
    "            'power': np.nan,\n",
    "            'significant': False,\n",
    "            'error': f'Mann-Whitney U error: {e}'\n",
    "        }\n",
    "\n",
    "\n",
    "    # Effect Size: Cohen's d\n",
    "    mean1, mean2 = np.mean(data1), np.mean(data2)\n",
    "    std1, std2 = np.std(data1, ddof=1), np.std(data2, ddof=1)\n",
    "    n1, n2 = len(data1), len(data2)\n",
    "    \n",
    "    if n1 + n2 - 2 <= 0: # Should be caught by len(data) < 2 but as an extra safe guard\n",
    "        cohen_d = np.nan\n",
    "    else:\n",
    "        s_pooled = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "        if s_pooled == 0:\n",
    "            cohen_d = np.inf if mean1 != mean2 else 0\n",
    "        else:\n",
    "            cohen_d = (mean1 - mean2) / s_pooled\n",
    "            \n",
    "    # Post-hoc Power\n",
    "    power = np.nan\n",
    "    if not np.isnan(cohen_d) and n1 > 0 and n2 > 0 : # Check if cohen_d is valid\n",
    "        power_analysis = TTestIndPower()\n",
    "        try:\n",
    "            power = power_analysis.power(effect_size=abs(cohen_d), \n",
    "                                         nobs1=n1, \n",
    "                                         ratio=n2/n1,\n",
    "                                         alpha=alpha_corrected)\n",
    "        except Exception: # Catch any error from power calculation\n",
    "            power = np.nan\n",
    "\n",
    "    significant = p_value < alpha_corrected\n",
    "    \n",
    "    return {\n",
    "        'p_value': p_value,\n",
    "        'cohen_d': cohen_d,\n",
    "        'power': power,\n",
    "        'significant': significant,\n",
    "        'n1': n1,\n",
    "        'n2': n2,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "# --- Main Statistical Analysis Loop ---\n",
    "alpha = 0.05 # Overall significance level\n",
    "\n",
    "for metric in test_metric_columns:\n",
    "    print(f\"\\n\\n--- Analyzing Metric: {metric} ---\")\n",
    "    \n",
    "    metric_df = df.dropna(subset=[metric]).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    \n",
    "    if metric_df.empty or metric_df['hpo_config_signature'].nunique() < 2:\n",
    "        print(f\"Skipping {metric} due to insufficient data or unique groups after NaN removal.\")\n",
    "        continue\n",
    "\n",
    "    # Kruskal-Wallis needs groups with at least one observation,\n",
    "    # but for meaningful comparison, more are needed.\n",
    "    # The perform_pairwise_comparison handles groups with < 2 observations.\n",
    "    grouped_data_for_kruskal = [\n",
    "        group[metric].values for _, group in metric_df.groupby('hpo_config_signature') if len(group[metric].values) > 0\n",
    "    ]\n",
    "    \n",
    "    if len(grouped_data_for_kruskal) < 2:\n",
    "        print(f\"Skipping Kruskal-Wallis for {metric} as fewer than 2 groups have data.\")\n",
    "        # Still attempt to show boxplot\n",
    "        if not metric_df.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            # Ensure hpo_config_signature_str is created for plotting if not already\n",
    "            if 'hpo_config_signature_str' not in metric_df.columns:\n",
    "                 metric_df['hpo_config_signature_str'] = metric_df['hpo_config_signature'].astype(str)\n",
    "            \n",
    "            sorted_configs_plot = metric_df.groupby('hpo_config_signature_str')[metric].median().sort_values().index\n",
    "            sns.boxplot(x='hpo_config_signature_str', y=metric, data=metric_df, order=sorted_configs_plot)\n",
    "            plt.title(f'Distribution of {metric} by HPO Configuration')\n",
    "            plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        continue\n",
    "        \n",
    "    kruskal_stat, kruskal_p_value = stats.kruskal(*grouped_data_for_kruskal)\n",
    "    print(f\"\\nKruskal-Wallis test for {metric}:\")\n",
    "    print(f\"  Statistic: {kruskal_stat:.4f}, P-value: {kruskal_p_value:.4f}\")\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    metric_df['hpo_config_signature_str'] = metric_df['hpo_config_signature'].astype(str)\n",
    "    sorted_configs_plot = metric_df.groupby('hpo_config_signature_str')[metric].median().sort_values().index\n",
    "    sns.boxplot(x='hpo_config_signature_str', y=metric, data=metric_df, order=sorted_configs_plot)\n",
    "    plt.title(f'Distribution of {metric} by HPO Configuration (Sorted by Median)')\n",
    "    plt.xlabel('HPO Configuration Signature')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if kruskal_p_value < alpha:\n",
    "        print(f\"  Significant difference found among HPO configurations for {metric} (p < {alpha}). Performing pairwise comparisons.\")\n",
    "        \n",
    "        configs = list(metric_df['hpo_config_signature'].unique())\n",
    "        n_configs = len(configs)\n",
    "        # n_comparisons calculation should only count valid pairs for Bonferroni\n",
    "        # For simplicity, we'll apply Bonferroni based on actual comparisons made.\n",
    "        \n",
    "        pairwise_results_list = []\n",
    "        \n",
    "        # Count actual comparisons to be made for Bonferroni\n",
    "        num_actual_comparisons = 0\n",
    "        temp_pairs_to_compare = []\n",
    "        for i in range(n_configs):\n",
    "            for j in range(i + 1, n_configs):\n",
    "                config1_sig_temp = configs[i]\n",
    "                config2_sig_temp = configs[j]\n",
    "                data1_temp = metric_df[metric_df['hpo_config_signature'] == config1_sig_temp][metric].dropna().values\n",
    "                data2_temp = metric_df[metric_df['hpo_config_signature'] == config2_sig_temp][metric].dropna().values\n",
    "                if len(data1_temp) >=2 and len(data2_temp) >=2:\n",
    "                    num_actual_comparisons +=1\n",
    "                    temp_pairs_to_compare.append((config1_sig_temp, config2_sig_temp))\n",
    "        \n",
    "        if num_actual_comparisons == 0:\n",
    "            print(\"  No valid pairs for pairwise comparison (e.g. all groups have < 2 data points).\")\n",
    "            continue\n",
    "            \n",
    "        corrected_alpha = alpha / num_actual_comparisons\n",
    "        print(f\"  Using Bonferroni corrected alpha for pairwise tests: {corrected_alpha:.5f} ({alpha}/{num_actual_comparisons})\")\n",
    "\n",
    "        for config1_sig, config2_sig in temp_pairs_to_compare:\n",
    "            data1 = metric_df[metric_df['hpo_config_signature'] == config1_sig][metric].dropna().values\n",
    "            data2 = metric_df[metric_df['hpo_config_signature'] == config2_sig][metric].dropna().values\n",
    "            \n",
    "            # Already checked len(data1) >=2 and len(data2) >=2\n",
    "            comparison_stats = perform_pairwise_comparison(data1, data2, corrected_alpha)\n",
    "            \n",
    "            if comparison_stats['error']:\n",
    "                print(f\"  Skipping {config1_sig} vs {config2_sig}: {comparison_stats['error']}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Comparison: {str(config1_sig)} vs {str(config2_sig)}\")\n",
    "            print(f\"    Mann-Whitney U p-value: {comparison_stats['p_value']:.4f} \"\n",
    "                  f\"(Significant w/ Bonferroni: {comparison_stats['significant']})\")\n",
    "            print(f\"    Cohen's d: {comparison_stats['cohen_d']:.3f}\")\n",
    "            print(f\"    Post-hoc Power (approx, alpha={corrected_alpha:.4f}): {comparison_stats['power']:.3f}\")\n",
    "            print(f\"    N1: {comparison_stats['n1']}, N2: {comparison_stats['n2']}\")\n",
    "            \n",
    "            pairwise_results_list.append({\n",
    "                'metric': metric,\n",
    "                'config1': str(config1_sig),\n",
    "                'config2': str(config2_sig),\n",
    "                'p_value': comparison_stats['p_value'],\n",
    "                'cohen_d': comparison_stats['cohen_d'],\n",
    "                'power': comparison_stats['power'],\n",
    "                'significant_bonferroni': comparison_stats['significant'],\n",
    "                'n1': comparison_stats['n1'],\n",
    "                'n2': comparison_stats['n2'],\n",
    "                'corrected_alpha': corrected_alpha\n",
    "            })\n",
    "\n",
    "        if pairwise_results_list:\n",
    "            pairwise_df = pd.DataFrame(pairwise_results_list)\n",
    "            print(f\"\\n  Summary of Pairwise Comparisons for {metric} (Bonferroni applied):\")\n",
    "            # Show all significant pairs, or pairs with large effect/high power\n",
    "            significant_or_notable = pairwise_df[\n",
    "                (pairwise_df['significant_bonferroni']) | \n",
    "                (pairwise_df['power'].fillna(0) > 0.8) | \n",
    "                (abs(pairwise_df['cohen_d'].fillna(0)) > 0.5)\n",
    "            ]\n",
    "            if not significant_or_notable.empty:\n",
    "                print(significant_or_notable.to_string(index=False, float_format=\"%.3f\"))\n",
    "            else:\n",
    "                print(\"    No pairs met the criteria for 'significant or notable' in the summary.\")\n",
    "    else:\n",
    "        print(f\"  No significant overall difference found among HPO configurations for {metric} (p >= {alpha}). Skipping pairwise comparisons.\")\n",
    "\n",
    "print(\"\\n\\nAnalysis Complete.\")\n",
    "print(\"Post-hoc power is descriptive. Effect sizes (Cohen's d) provide context to p-values.\")\n",
    "\n",
    "# --- Example: How to use perform_pairwise_comparison for two specific configurations ---\n",
    "# This is a conceptual example. You'll need to pick actual config signatures from your data.\n",
    "\n",
    "# First, ensure you have at least one metric processed and unique_configs populated\n",
    "if test_metric_columns and 'hpo_config_signature' in df.columns and len(df['hpo_config_signature'].unique()) >=2 :\n",
    "    example_metric = test_metric_columns[0] # Take the first test metric\n",
    "    print(f\"\\n\\n--- Example of Specific Pairwise Comparison for metric: {example_metric} ---\")\n",
    "    \n",
    "    # Get all unique configurations for this metric after NaN removal\n",
    "    example_metric_df = df.dropna(subset=[example_metric])\n",
    "    available_configs_for_example = list(example_metric_df['hpo_config_signature'].unique())\n",
    "\n",
    "    if len(available_configs_for_example) >= 2:\n",
    "        config_A_sig = available_configs_for_example[0]\n",
    "        config_B_sig = available_configs_for_example[1]\n",
    "\n",
    "        print(f\"Comparing Config A: {config_A_sig}\")\n",
    "        print(f\"with Config B: {config_B_sig}\")\n",
    "\n",
    "        data_A = example_metric_df[example_metric_df['hpo_config_signature'] == config_A_sig][example_metric].dropna().values\n",
    "        data_B = example_metric_df[example_metric_df['hpo_config_signature'] == config_B_sig][example_metric].dropna().values\n",
    "        \n",
    "        # For a standalone comparison, you might use the original alpha or a corrected one\n",
    "        # depending on context (e.g., if this is one of many planned comparisons).\n",
    "        # Here, using original alpha for a single, ad-hoc comparison example.\n",
    "        standalone_alpha = 0.05 \n",
    "        \n",
    "        specific_comparison_results = perform_pairwise_comparison(data_A, data_B, alpha_corrected=standalone_alpha)\n",
    "\n",
    "        if specific_comparison_results['error']:\n",
    "             print(f\"Could not compare: {specific_comparison_results['error']}\")\n",
    "        else:\n",
    "            print(f\"  Metric: {example_metric}\")\n",
    "            print(f\"  Mann-Whitney U p-value: {specific_comparison_results['p_value']:.4f}\")\n",
    "            print(f\"  Cohen's d: {specific_comparison_results['cohen_d']:.3f}\")\n",
    "            print(f\"  Power (alpha={standalone_alpha}): {specific_comparison_results['power']:.3f}\")\n",
    "            print(f\"  Significant at alpha={standalone_alpha}: {specific_comparison_results['significant']}\")\n",
    "            print(f\"  N_A: {specific_comparison_results['n1']}, N_B: {specific_comparison_results['n2']}\")\n",
    "    else:\n",
    "        print(\"Not enough unique configurations with data for the example metric to perform a specific pairwise comparison.\")\n",
    "else:\n",
    "    print(\"\\nSkipping example of specific pairwise comparison due to lack of processed metrics or configurations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
