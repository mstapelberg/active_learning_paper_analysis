{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : I need to compile and test all of the models on an identical testset, run a cross-validation on the top 10 models from an accuracy standpoint, like I did for MACE. \n",
    "\n",
    "Currently, I only have the validation results for the models, not the test results, I should modify the evaluate_config.py to also accept an allegro model in addition to a MACE model. \n",
    "\n",
    "Additionally, I need to refactor FORGE to do db to MLIP better, get the config_type working in the database, and fully use the forge workflow to deploy generation 7 models (instead of using my manual deployment process). Also need to see why the energy, force, and stress get saved in the metadata section as well as the calculation table \n",
    "\n",
    "1. get all of the OUTCARs from gen 7\n",
    "2. get them into an xyz file \n",
    "3. add them to the database with the correct metadata\n",
    "4. deploy MACE MLIP training script on PSFC-GPU cluster with pair-repulsion added \n",
    "5. train the 5 models in the ensemble \n",
    "6. use generation 7 models to do Adversarial Attack for generation 8 model --> make 2000 more datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'State', 'Notes', 'User', 'Tags', 'Created', 'Runtime', 'Sweep',\n",
      "       'AllegroBesselBasis_trainable', 'PolynomialCutoff_p',\n",
      "       ...\n",
      "       'validation_e_mae', 'validation_f_mae', 'validation_f_rmse',\n",
      "       'validation_loss', 'validation_loss_e', 'validation_loss_f',\n",
      "       'validation_loss_stress', 'validation_stress_mae',\n",
      "       'validation_stress_rmse', 'wall'],\n",
      "      dtype='object', length=109)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/hpo_analysis/allegro_wandb_export_2025-03-18T23_09_11.553-04_00.csv')\n",
    "\n",
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allegro_model_161_r_max8p0_l_max2_num_layers2_num_tensor_features64_learning_rate0p001\n"
     ]
    }
   ],
   "source": [
    "print(df['Name'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
