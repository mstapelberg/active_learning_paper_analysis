{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup & Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0· Imports & paths\n",
    "# -----------------------------------------------------------\n",
    "import re, ast, textwrap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "ROOT = Path(\"_aggregated\")          # adjust if untarred elsewhere\n",
    "METRICS_F = ROOT / \"combined_metrics.parquet\"\n",
    "SUM_F     = ROOT / \"summaries.parquet\"        # optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Exploration & Intelligent Merging\n",
    "\n",
    "This section explores both dataframes, identifies overlapping columns, and merges them intelligently:\n",
    "- **Preserves all data**: When values differ between dataframes, keeps both with suffixes\n",
    "- **Avoids duplication**: When values are identical, keeps only one copy\n",
    "- **No data loss**: Every piece of information is retained for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METRICS DATAFRAME ===\n",
      "Shape: (79, 122)\n",
      "Columns: 122\n",
      "Sample run IDs: ['base_huber_q0.9_no_sampler' 'base_huber_q0.9_rare_sampling'\n",
      " 'huber_q0.75_no_sampler' 'huber_q0.75_rare_sampling'\n",
      " 'huber_q0.85_no_sampler']\n",
      "\n",
      "=== SUMMARY DATAFRAME ===\n",
      "Shape: (79, 121)\n",
      "Columns: 121\n",
      "Sample run IDs: ['base_huber_q0.9_no_sampler' 'base_huber_q0.9_rare_sampling'\n",
      " 'huber_q0.75_no_sampler' 'huber_q0.75_rare_sampling'\n",
      " 'huber_q0.85_no_sampler']\n",
      "\n",
      "=== OVERLAPPING COLUMNS (120) ===\n",
      "  _runtime\n",
      "  _step\n",
      "  _timestamp\n",
      "  _wandb\n",
      "  cfg_ema_decay\n",
      "  cfg_gradient_clip_algorithm\n",
      "  cfg_gradient_clip_val\n",
      "  cfg_info_dict\n",
      "  cfg_loss\n",
      "  cfg_lr_scheduler\n",
      "  cfg_lsqr\n",
      "  cfg_model\n",
      "  cfg_norm_eps\n",
      "  cfg_num_datasets\n",
      "  cfg_optimizer\n",
      "  cfg_test_metrics\n",
      "  cfg_train_metrics\n",
      "  cfg_val_metrics\n",
      "  epoch\n",
      "  loss_coeff/force_strat_E_huber\n",
      "  loss_coeff/forces_angle_force_angle\n",
      "  loss_coeff/forces_angle_forces_angle\n",
      "  loss_coeff/forces_auto_stratified_huber\n",
      "  loss_coeff/forces_focal_focal_mse\n",
      "  loss_coeff/forces_huber\n",
      "  loss_coeff/forces_mse\n",
      "  loss_coeff/forces_stratified_huber\n",
      "  loss_coeff/forces_tail_huber\n",
      "  loss_coeff/per_atom_energy_huber\n",
      "  loss_coeff/per_atom_energy_mse\n",
      "  loss_coeff/peratom_E_Huber\n",
      "  loss_coeff/stress_E_Huber\n",
      "  loss_coeff/stress_angle_stress_angle\n",
      "  loss_coeff/stress_huber\n",
      "  loss_coeff/stress_mse\n",
      "  loss_coeff/stress_shear_stress_shear_mae\n",
      "  lr-Adam\n",
      "  lr-AdamW\n",
      "  project\n",
      "  run\n",
      "  test0_epoch/forces_mae\n",
      "  test0_epoch/forces_rmse\n",
      "  test0_epoch/per_atom_energy_mae\n",
      "  test0_epoch/per_atom_energy_rmse\n",
      "  test0_epoch/stress_mae\n",
      "  test0_epoch/stress_rmse\n",
      "  test0_epoch/total_energy_mae\n",
      "  test0_epoch/total_energy_rmse\n",
      "  test0_epoch/weighted_sum\n",
      "  train_loss/auto_delta\n",
      "  train_loss_epoch/force_strat_E_huber\n",
      "  train_loss_epoch/forces_angle_force_angle\n",
      "  train_loss_epoch/forces_angle_forces_angle\n",
      "  train_loss_epoch/forces_auto_stratified_huber\n",
      "  train_loss_epoch/forces_focal_focal_mse\n",
      "  train_loss_epoch/forces_huber\n",
      "  train_loss_epoch/forces_mse\n",
      "  train_loss_epoch/forces_stratified_huber\n",
      "  train_loss_epoch/forces_tail_huber\n",
      "  train_loss_epoch/per_atom_energy_huber\n",
      "  train_loss_epoch/per_atom_energy_mse\n",
      "  train_loss_epoch/peratom_E_Huber\n",
      "  train_loss_epoch/stress_E_Huber\n",
      "  train_loss_epoch/stress_angle_stress_angle\n",
      "  train_loss_epoch/stress_huber\n",
      "  train_loss_epoch/stress_mse\n",
      "  train_loss_epoch/stress_shear_stress_shear_mae\n",
      "  train_loss_epoch/weighted_sum\n",
      "  train_loss_step/force_strat_E_huber\n",
      "  train_loss_step/forces_angle_force_angle\n",
      "  train_loss_step/forces_angle_forces_angle\n",
      "  train_loss_step/forces_auto_stratified_huber\n",
      "  train_loss_step/forces_focal_focal_mse\n",
      "  train_loss_step/forces_huber\n",
      "  train_loss_step/forces_mse\n",
      "  train_loss_step/forces_stratified_huber\n",
      "  train_loss_step/forces_tail_huber\n",
      "  train_loss_step/per_atom_energy_huber\n",
      "  train_loss_step/per_atom_energy_mse\n",
      "  train_loss_step/peratom_E_Huber\n",
      "  train_loss_step/stress_E_Huber\n",
      "  train_loss_step/stress_angle_stress_angle\n",
      "  train_loss_step/stress_huber\n",
      "  train_loss_step/stress_mse\n",
      "  train_loss_step/stress_shear_stress_shear_mae\n",
      "  train_loss_step/weighted_sum\n",
      "  train_metric_epoch/forces_mae\n",
      "  train_metric_epoch/forces_rmse\n",
      "  train_metric_epoch/per_atom_energy_mae\n",
      "  train_metric_epoch/per_atom_energy_rmse\n",
      "  train_metric_epoch/stress_mae\n",
      "  train_metric_epoch/stress_rmse\n",
      "  train_metric_epoch/total_energy_mae\n",
      "  train_metric_epoch/total_energy_rmse\n",
      "  train_metric_epoch/weighted_sum\n",
      "  train_metric_step/forces_mae\n",
      "  train_metric_step/forces_rmse\n",
      "  train_metric_step/per_atom_energy_mae\n",
      "  train_metric_step/per_atom_energy_rmse\n",
      "  train_metric_step/stress_mae\n",
      "  train_metric_step/stress_rmse\n",
      "  train_metric_step/total_energy_mae\n",
      "  train_metric_step/total_energy_rmse\n",
      "  train_metric_step/weighted_sum\n",
      "  trainer/global_step\n",
      "  val0_epoch/forces_rmse\n",
      "  val0_epoch/per_atom_energy_mae\n",
      "  val0_epoch/per_atom_energy_rmse\n",
      "  val0_epoch/stress_mae\n",
      "  val0_epoch/stress_rmse\n",
      "  val0_epoch/total_energy_mae\n",
      "  val0_epoch/total_energy_rmse\n",
      "  val0_epoch/weighted_sum\n",
      "  val1_epoch/forces_mae\n",
      "  val1_epoch/forces_rmse\n",
      "  val1_epoch/per_atom_energy_mae\n",
      "  val1_epoch/per_atom_energy_rmse\n",
      "  val1_epoch/stress_mae\n",
      "  val1_epoch/stress_rmse\n",
      "  val1_epoch/weighted_sum\n",
      "\n",
      "=== METRICS-ONLY COLUMNS (2) ===\n",
      "  val0_epoch/forces_mae_x\n",
      "  val0_epoch/forces_mae_y\n",
      "\n",
      "=== SUMMARY-ONLY COLUMNS (1) ===\n",
      "  val0_epoch/forces_mae\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1· DATAFRAME EXPLORATION & COMPARISON\n",
    "# -----------------------------------------------------------\n",
    "metrics = pd.read_parquet(METRICS_F)\n",
    "summary = pd.read_parquet(SUM_F) if SUM_F.exists() else None\n",
    "\n",
    "print(\"=== METRICS DATAFRAME ===\")\n",
    "print(f\"Shape: {metrics.shape}\")\n",
    "print(f\"Columns: {len(metrics.columns)}\")\n",
    "print(f\"Sample run IDs: {metrics['run'].unique()[:5]}\")\n",
    "\n",
    "if summary is not None:\n",
    "    print(\"\\n=== SUMMARY DATAFRAME ===\")\n",
    "    print(f\"Shape: {summary.shape}\")\n",
    "    print(f\"Columns: {len(summary.columns)}\")\n",
    "    print(f\"Sample run IDs: {summary['run'].unique()[:5]}\")\n",
    "    \n",
    "    # Find overlapping columns\n",
    "    overlapping_cols = set(metrics.columns) & set(summary.columns)\n",
    "    print(f\"\\n=== OVERLAPPING COLUMNS ({len(overlapping_cols)}) ===\")\n",
    "    for col in sorted(overlapping_cols):\n",
    "        print(f\"  {col}\")\n",
    "        \n",
    "    # Find unique columns in each\n",
    "    metrics_only = set(metrics.columns) - set(summary.columns)\n",
    "    summary_only = set(summary.columns) - set(metrics.columns)\n",
    "    \n",
    "    print(f\"\\n=== METRICS-ONLY COLUMNS ({len(metrics_only)}) ===\")\n",
    "    for col in sorted(list(metrics_only)[:15]):  # Show first 15\n",
    "        print(f\"  {col}\")\n",
    "    if len(metrics_only) > 15:\n",
    "        print(f\"  ... and {len(metrics_only) - 15} more\")\n",
    "        \n",
    "    print(f\"\\n=== SUMMARY-ONLY COLUMNS ({len(summary_only)}) ===\")\n",
    "    for col in sorted(list(summary_only)[:15]):  # Show first 15\n",
    "        print(f\"  {col}\")\n",
    "    if len(summary_only) > 15:\n",
    "        print(f\"  ... and {len(summary_only) - 15} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARING VALUES FOR OVERLAPPING COLUMNS ===\n",
      "\n",
      "--- Column: loss_coeff/force_strat_E_huber ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: train_metric_epoch/total_energy_mae ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: test0_epoch/total_energy_rmse ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: train_loss_step/forces_stratified_huber ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: train_metric_epoch/per_atom_energy_rmse ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: train_loss_step/forces_tail_huber ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 2/13\n",
      "    Run huber_q0.85_rare_sampling: metrics=0.009536574100866387, summary=0.0653028466144636\n",
      "    Run huber_q0.85_rare_sampling: metrics=0.0653028466144636, summary=0.009536574100866387\n",
      "\n",
      "--- Column: train_loss_step/forces_angle_forces_angle ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: loss_coeff/forces_angle_force_angle ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "--- Column: train_loss_step/weighted_sum ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 2/13\n",
      "    Run huber_q0.85_rare_sampling: metrics=0.0015749672158386688, summary=0.019493512966554095\n",
      "    Run huber_q0.85_rare_sampling: metrics=0.019493512966554095, summary=0.0015749672158386688\n",
      "\n",
      "--- Column: train_loss_epoch/stress_E_Huber ---\n",
      "  ⚠️  DIFFERENCES FOUND!\n",
      "  Runs with differences: 13/13\n",
      "    Run base_huber_q0.9_no_sampler: metrics=nan, summary=nan\n",
      "    Run base_huber_q0.9_rare_sampling: metrics=nan, summary=nan\n",
      "    Run huber_q0.75_no_sampler: metrics=nan, summary=nan\n",
      "\n",
      "=== SUMMARY OF DIFFERENCES ===\n",
      "Columns with differences: 10\n",
      "  - loss_coeff/force_strat_E_huber\n",
      "  - train_metric_epoch/total_energy_mae\n",
      "  - test0_epoch/total_energy_rmse\n",
      "  - train_loss_step/forces_stratified_huber\n",
      "  - train_metric_epoch/per_atom_energy_rmse\n",
      "  - train_loss_step/forces_tail_huber\n",
      "  - train_loss_step/forces_angle_forces_angle\n",
      "  - loss_coeff/forces_angle_force_angle\n",
      "  - train_loss_step/weighted_sum\n",
      "  - train_loss_epoch/stress_E_Huber\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2· COMPARE VALUES FOR OVERLAPPING COLUMNS\n",
    "# -----------------------------------------------------------\n",
    "if summary is not None:\n",
    "    # Get overlapping columns (excluding 'run' which is the join key)\n",
    "    overlapping_cols = [col for col in set(metrics.columns) & set(summary.columns) if col != 'run']\n",
    "    \n",
    "    print(\"=== COMPARING VALUES FOR OVERLAPPING COLUMNS ===\")\n",
    "    differences_found = {}\n",
    "    \n",
    "    # Sample some runs for comparison\n",
    "    sample_runs = metrics['run'].unique()[:10]  # Check first 10 runs\n",
    "    \n",
    "    for col in overlapping_cols[:10]:  # Check first 10 overlapping columns\n",
    "        print(f\"\\n--- Column: {col} ---\")\n",
    "        \n",
    "        # Get values for sample runs\n",
    "        metrics_sample = metrics[metrics['run'].isin(sample_runs)][['run', col]].set_index('run')\n",
    "        summary_sample = summary[summary['run'].isin(sample_runs)][['run', col]].set_index('run')\n",
    "        \n",
    "        # Compare values\n",
    "        comparison = metrics_sample.join(summary_sample, how='inner', lsuffix='_metrics', rsuffix='_summary')\n",
    "        \n",
    "        # Check if values are different\n",
    "        if col + '_metrics' in comparison.columns and col + '_summary' in comparison.columns:\n",
    "            # Handle different data types\n",
    "            try:\n",
    "                different_mask = comparison[col + '_metrics'] != comparison[col + '_summary']\n",
    "                if different_mask.any():\n",
    "                    differences_found[col] = True\n",
    "                    print(f\"  ⚠️  DIFFERENCES FOUND!\")\n",
    "                    print(f\"  Runs with differences: {different_mask.sum()}/{len(different_mask)}\")\n",
    "                    \n",
    "                    # Show a few examples\n",
    "                    diff_examples = comparison[different_mask].head(3)\n",
    "                    for run_id, row in diff_examples.iterrows():\n",
    "                        print(f\"    Run {run_id}: metrics={row[col + '_metrics']}, summary={row[col + '_summary']}\")\n",
    "                else:\n",
    "                    print(f\"  ✅ No differences found\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❓ Could not compare (different types): {e}\")\n",
    "        else:\n",
    "            print(f\"  ❓ Comparison issue - column structure unexpected\")\n",
    "    \n",
    "    print(f\"\\n=== SUMMARY OF DIFFERENCES ===\")\n",
    "    print(f\"Columns with differences: {len(differences_found)}\")\n",
    "    if differences_found:\n",
    "        for col in differences_found:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"No differences found in sampled data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== APPLYING SMART MERGE ===\n",
      "Column 'loss_coeff/force_strat_E_huber': Values differ -> keeping both as loss_coeff/force_strat_E_huber_metrics and loss_coeff/force_strat_E_huber_summary\n",
      "Column 'train_metric_epoch/total_energy_mae': Values differ -> keeping both as train_metric_epoch/total_energy_mae_metrics and train_metric_epoch/total_energy_mae_summary\n",
      "Column 'test0_epoch/total_energy_rmse': Values differ -> keeping both as test0_epoch/total_energy_rmse_metrics and test0_epoch/total_energy_rmse_summary\n",
      "Column 'train_loss_step/forces_stratified_huber': Values differ -> keeping both as train_loss_step/forces_stratified_huber_metrics and train_loss_step/forces_stratified_huber_summary\n",
      "Column 'train_metric_epoch/per_atom_energy_rmse': Values differ -> keeping both as train_metric_epoch/per_atom_energy_rmse_metrics and train_metric_epoch/per_atom_energy_rmse_summary\n",
      "Column 'train_loss_step/forces_tail_huber': Values differ -> keeping both as train_loss_step/forces_tail_huber_metrics and train_loss_step/forces_tail_huber_summary\n",
      "Column 'train_loss_step/forces_angle_forces_angle': Values differ -> keeping both as train_loss_step/forces_angle_forces_angle_metrics and train_loss_step/forces_angle_forces_angle_summary\n",
      "Column 'loss_coeff/forces_angle_force_angle': Values differ -> keeping both as loss_coeff/forces_angle_force_angle_metrics and loss_coeff/forces_angle_force_angle_summary\n",
      "Column 'train_loss_step/weighted_sum': Values differ -> keeping both as train_loss_step/weighted_sum_metrics and train_loss_step/weighted_sum_summary\n",
      "Column 'train_loss_epoch/stress_E_Huber': Values differ -> keeping both as train_loss_epoch/stress_E_Huber_metrics and train_loss_epoch/stress_E_Huber_summary\n",
      "Column 'test0_epoch/per_atom_energy_mae': Values differ -> keeping both as test0_epoch/per_atom_energy_mae_metrics and test0_epoch/per_atom_energy_mae_summary\n",
      "Column 'train_loss/auto_delta': Values differ -> keeping both as train_loss/auto_delta_metrics and train_loss/auto_delta_summary\n",
      "Column 'trainer/global_step': Values differ -> keeping both as trainer/global_step_metrics and trainer/global_step_summary\n",
      "Column 'test0_epoch/forces_mae': Values differ -> keeping both as test0_epoch/forces_mae_metrics and test0_epoch/forces_mae_summary\n",
      "Column 'train_metric_step/stress_mae': Values differ -> keeping both as train_metric_step/stress_mae_metrics and train_metric_step/stress_mae_summary\n",
      "Column 'cfg_lsqr': Values differ -> keeping both as cfg_lsqr_metrics and cfg_lsqr_summary\n",
      "Column 'loss_coeff/forces_auto_stratified_huber': Values differ -> keeping both as loss_coeff/forces_auto_stratified_huber_metrics and loss_coeff/forces_auto_stratified_huber_summary\n",
      "Column 'train_loss_step/stress_mse': Values differ -> keeping both as train_loss_step/stress_mse_metrics and train_loss_step/stress_mse_summary\n",
      "Column 'train_loss_epoch/forces_focal_focal_mse': Values differ -> keeping both as train_loss_epoch/forces_focal_focal_mse_metrics and train_loss_epoch/forces_focal_focal_mse_summary\n",
      "Column 'train_metric_step/per_atom_energy_mae': Values differ -> keeping both as train_metric_step/per_atom_energy_mae_metrics and train_metric_step/per_atom_energy_mae_summary\n",
      "Column 'loss_coeff/stress_angle_stress_angle': Values differ -> keeping both as loss_coeff/stress_angle_stress_angle_metrics and loss_coeff/stress_angle_stress_angle_summary\n",
      "Column 'val0_epoch/per_atom_energy_mae': Values differ -> keeping both as val0_epoch/per_atom_energy_mae_metrics and val0_epoch/per_atom_energy_mae_summary\n",
      "Column 'train_metric_step/weighted_sum': Values differ -> keeping both as train_metric_step/weighted_sum_metrics and train_metric_step/weighted_sum_summary\n",
      "Column 'val0_epoch/weighted_sum': Values differ -> keeping both as val0_epoch/weighted_sum_metrics and val0_epoch/weighted_sum_summary\n",
      "Column 'cfg_train_metrics': Values differ -> keeping both as cfg_train_metrics_metrics and cfg_train_metrics_summary\n",
      "Column 'val0_epoch/per_atom_energy_rmse': Values differ -> keeping both as val0_epoch/per_atom_energy_rmse_metrics and val0_epoch/per_atom_energy_rmse_summary\n",
      "Column 'train_loss_epoch/forces_stratified_huber': Values differ -> keeping both as train_loss_epoch/forces_stratified_huber_metrics and train_loss_epoch/forces_stratified_huber_summary\n",
      "Column 'lr-AdamW': Values differ -> keeping both as lr-AdamW_metrics and lr-AdamW_summary\n",
      "Column 'cfg_loss': Values differ -> keeping both as cfg_loss_metrics and cfg_loss_summary\n",
      "Column 'cfg_test_metrics': Values identical -> keeping metrics version only\n",
      "Column 'epoch': Values differ -> keeping both as epoch_metrics and epoch_summary\n",
      "Column 'train_loss_step/stress_angle_stress_angle': Values differ -> keeping both as train_loss_step/stress_angle_stress_angle_metrics and train_loss_step/stress_angle_stress_angle_summary\n",
      "Column 'train_loss_epoch/forces_huber': Values differ -> keeping both as train_loss_epoch/forces_huber_metrics and train_loss_epoch/forces_huber_summary\n",
      "Column 'train_loss_epoch/stress_huber': Values differ -> keeping both as train_loss_epoch/stress_huber_metrics and train_loss_epoch/stress_huber_summary\n",
      "Column '_runtime': Values differ -> keeping both as _runtime_metrics and _runtime_summary\n",
      "Column 'train_loss_epoch/per_atom_energy_huber': Values differ -> keeping both as train_loss_epoch/per_atom_energy_huber_metrics and train_loss_epoch/per_atom_energy_huber_summary\n",
      "Column 'train_metric_epoch/per_atom_energy_mae': Values differ -> keeping both as train_metric_epoch/per_atom_energy_mae_metrics and train_metric_epoch/per_atom_energy_mae_summary\n",
      "Column 'cfg_optimizer': Values identical -> keeping metrics version only\n",
      "Column 'train_metric_step/forces_mae': Values differ -> keeping both as train_metric_step/forces_mae_metrics and train_metric_step/forces_mae_summary\n",
      "Column 'test0_epoch/total_energy_mae': Values differ -> keeping both as test0_epoch/total_energy_mae_metrics and test0_epoch/total_energy_mae_summary\n",
      "Column 'cfg_val_metrics': Values identical -> keeping metrics version only\n",
      "Column 'val1_epoch/weighted_sum': Values differ -> keeping both as val1_epoch/weighted_sum_metrics and val1_epoch/weighted_sum_summary\n",
      "Column 'train_loss_step/forces_focal_focal_mse': Values differ -> keeping both as train_loss_step/forces_focal_focal_mse_metrics and train_loss_step/forces_focal_focal_mse_summary\n",
      "Column 'train_loss_step/forces_huber': Values differ -> keeping both as train_loss_step/forces_huber_metrics and train_loss_step/forces_huber_summary\n",
      "Column 'val1_epoch/stress_rmse': Values differ -> keeping both as val1_epoch/stress_rmse_metrics and val1_epoch/stress_rmse_summary\n",
      "Column 'train_metric_step/total_energy_mae': Values differ -> keeping both as train_metric_step/total_energy_mae_metrics and train_metric_step/total_energy_mae_summary\n",
      "Column 'train_metric_step/total_energy_rmse': Values differ -> keeping both as train_metric_step/total_energy_rmse_metrics and train_metric_step/total_energy_rmse_summary\n",
      "Column 'lr-Adam': Values differ -> keeping both as lr-Adam_metrics and lr-Adam_summary\n",
      "Column 'val0_epoch/total_energy_mae': Values differ -> keeping both as val0_epoch/total_energy_mae_metrics and val0_epoch/total_energy_mae_summary\n",
      "Column 'train_loss_epoch/force_strat_E_huber': Values differ -> keeping both as train_loss_epoch/force_strat_E_huber_metrics and train_loss_epoch/force_strat_E_huber_summary\n",
      "Column 'train_loss_epoch/stress_shear_stress_shear_mae': Values differ -> keeping both as train_loss_epoch/stress_shear_stress_shear_mae_metrics and train_loss_epoch/stress_shear_stress_shear_mae_summary\n",
      "Column 'train_metric_epoch/total_energy_rmse': Values differ -> keeping both as train_metric_epoch/total_energy_rmse_metrics and train_metric_epoch/total_energy_rmse_summary\n",
      "Column 'train_loss_step/force_strat_E_huber': Values differ -> keeping both as train_loss_step/force_strat_E_huber_metrics and train_loss_step/force_strat_E_huber_summary\n",
      "Column 'train_loss_step/forces_mse': Values differ -> keeping both as train_loss_step/forces_mse_metrics and train_loss_step/forces_mse_summary\n",
      "Column '_timestamp': Values differ -> keeping both as _timestamp_metrics and _timestamp_summary\n",
      "Column 'cfg_gradient_clip_val': Values differ -> keeping both as cfg_gradient_clip_val_metrics and cfg_gradient_clip_val_summary\n",
      "Column 'train_loss_step/per_atom_energy_huber': Values differ -> keeping both as train_loss_step/per_atom_energy_huber_metrics and train_loss_step/per_atom_energy_huber_summary\n",
      "Column 'cfg_info_dict': Values differ -> keeping both as cfg_info_dict_metrics and cfg_info_dict_summary\n",
      "Column 'project': Values differ -> keeping both as project_metrics and project_summary\n",
      "Column 'test0_epoch/forces_rmse': Values differ -> keeping both as test0_epoch/forces_rmse_metrics and test0_epoch/forces_rmse_summary\n",
      "Column 'train_metric_epoch/weighted_sum': Values differ -> keeping both as train_metric_epoch/weighted_sum_metrics and train_metric_epoch/weighted_sum_summary\n",
      "Column 'cfg_norm_eps': Values differ -> keeping both as cfg_norm_eps_metrics and cfg_norm_eps_summary\n",
      "Column 'train_loss_epoch/forces_angle_forces_angle': Values differ -> keeping both as train_loss_epoch/forces_angle_forces_angle_metrics and train_loss_epoch/forces_angle_forces_angle_summary\n",
      "Column 'train_metric_step/per_atom_energy_rmse': Values differ -> keeping both as train_metric_step/per_atom_energy_rmse_metrics and train_metric_step/per_atom_energy_rmse_summary\n",
      "Column 'train_loss_epoch/per_atom_energy_mse': Values differ -> keeping both as train_loss_epoch/per_atom_energy_mse_metrics and train_loss_epoch/per_atom_energy_mse_summary\n",
      "Column 'loss_coeff/stress_mse': Values differ -> keeping both as loss_coeff/stress_mse_metrics and loss_coeff/stress_mse_summary\n",
      "Column 'train_loss_step/stress_huber': Values differ -> keeping both as train_loss_step/stress_huber_metrics and train_loss_step/stress_huber_summary\n",
      "Column 'loss_coeff/forces_stratified_huber': Values differ -> keeping both as loss_coeff/forces_stratified_huber_metrics and loss_coeff/forces_stratified_huber_summary\n",
      "Column 'cfg_num_datasets': Values differ -> keeping both as cfg_num_datasets_metrics and cfg_num_datasets_summary\n",
      "Column 'train_loss_step/peratom_E_Huber': Values differ -> keeping both as train_loss_step/peratom_E_Huber_metrics and train_loss_step/peratom_E_Huber_summary\n",
      "Column 'train_metric_epoch/forces_rmse': Values differ -> keeping both as train_metric_epoch/forces_rmse_metrics and train_metric_epoch/forces_rmse_summary\n",
      "Column 'loss_coeff/per_atom_energy_huber': Values differ -> keeping both as loss_coeff/per_atom_energy_huber_metrics and loss_coeff/per_atom_energy_huber_summary\n",
      "Column 'train_metric_step/stress_rmse': Values differ -> keeping both as train_metric_step/stress_rmse_metrics and train_metric_step/stress_rmse_summary\n",
      "Column '_step': Values differ -> keeping both as _step_metrics and _step_summary\n",
      "Column 'val1_epoch/stress_mae': Values differ -> keeping both as val1_epoch/stress_mae_metrics and val1_epoch/stress_mae_summary\n",
      "Column 'train_loss_epoch/forces_tail_huber': Values differ -> keeping both as train_loss_epoch/forces_tail_huber_metrics and train_loss_epoch/forces_tail_huber_summary\n",
      "Column 'val1_epoch/forces_rmse': Values differ -> keeping both as val1_epoch/forces_rmse_metrics and val1_epoch/forces_rmse_summary\n",
      "Column 'train_loss_epoch/stress_angle_stress_angle': Values differ -> keeping both as train_loss_epoch/stress_angle_stress_angle_metrics and train_loss_epoch/stress_angle_stress_angle_summary\n",
      "Column 'loss_coeff/forces_huber': Values differ -> keeping both as loss_coeff/forces_huber_metrics and loss_coeff/forces_huber_summary\n",
      "Column 'train_loss_epoch/stress_mse': Values differ -> keeping both as train_loss_epoch/stress_mse_metrics and train_loss_epoch/stress_mse_summary\n",
      "Column 'val1_epoch/forces_mae': Values differ -> keeping both as val1_epoch/forces_mae_metrics and val1_epoch/forces_mae_summary\n",
      "Column 'loss_coeff/peratom_E_Huber': Values differ -> keeping both as loss_coeff/peratom_E_Huber_metrics and loss_coeff/peratom_E_Huber_summary\n",
      "Column 'train_loss_step/per_atom_energy_mse': Values differ -> keeping both as train_loss_step/per_atom_energy_mse_metrics and train_loss_step/per_atom_energy_mse_summary\n",
      "Column 'test0_epoch/stress_mae': Values differ -> keeping both as test0_epoch/stress_mae_metrics and test0_epoch/stress_mae_summary\n",
      "Column 'train_loss_epoch/forces_auto_stratified_huber': Values differ -> keeping both as train_loss_epoch/forces_auto_stratified_huber_metrics and train_loss_epoch/forces_auto_stratified_huber_summary\n",
      "Column 'loss_coeff/stress_huber': Values differ -> keeping both as loss_coeff/stress_huber_metrics and loss_coeff/stress_huber_summary\n",
      "Column 'train_loss_epoch/forces_mse': Values differ -> keeping both as train_loss_epoch/forces_mse_metrics and train_loss_epoch/forces_mse_summary\n",
      "Column 'loss_coeff/per_atom_energy_mse': Values differ -> keeping both as loss_coeff/per_atom_energy_mse_metrics and loss_coeff/per_atom_energy_mse_summary\n",
      "Column 'val0_epoch/stress_mae': Values differ -> keeping both as val0_epoch/stress_mae_metrics and val0_epoch/stress_mae_summary\n",
      "Column 'cfg_ema_decay': Values identical -> keeping metrics version only\n",
      "Column 'cfg_model': Values differ -> keeping both as cfg_model_metrics and cfg_model_summary\n",
      "Column '_wandb': Values differ -> keeping both as _wandb_metrics and _wandb_summary\n",
      "Column 'test0_epoch/per_atom_energy_rmse': Values differ -> keeping both as test0_epoch/per_atom_energy_rmse_metrics and test0_epoch/per_atom_energy_rmse_summary\n",
      "Column 'train_metric_epoch/stress_rmse': Values differ -> keeping both as train_metric_epoch/stress_rmse_metrics and train_metric_epoch/stress_rmse_summary\n",
      "Column 'test0_epoch/stress_rmse': Values differ -> keeping both as test0_epoch/stress_rmse_metrics and test0_epoch/stress_rmse_summary\n",
      "Column 'cfg_gradient_clip_algorithm': Values differ -> keeping both as cfg_gradient_clip_algorithm_metrics and cfg_gradient_clip_algorithm_summary\n",
      "Column 'train_metric_epoch/forces_mae': Values differ -> keeping both as train_metric_epoch/forces_mae_metrics and train_metric_epoch/forces_mae_summary\n",
      "Column 'val0_epoch/forces_rmse': Values differ -> keeping both as val0_epoch/forces_rmse_metrics and val0_epoch/forces_rmse_summary\n",
      "Column 'loss_coeff/stress_E_Huber': Values differ -> keeping both as loss_coeff/stress_E_Huber_metrics and loss_coeff/stress_E_Huber_summary\n",
      "Column 'loss_coeff/stress_shear_stress_shear_mae': Values differ -> keeping both as loss_coeff/stress_shear_stress_shear_mae_metrics and loss_coeff/stress_shear_stress_shear_mae_summary\n",
      "Column 'train_loss_step/stress_E_Huber': Values differ -> keeping both as train_loss_step/stress_E_Huber_metrics and train_loss_step/stress_E_Huber_summary\n",
      "Column 'loss_coeff/forces_focal_focal_mse': Values differ -> keeping both as loss_coeff/forces_focal_focal_mse_metrics and loss_coeff/forces_focal_focal_mse_summary\n",
      "Column 'train_metric_step/forces_rmse': Values differ -> keeping both as train_metric_step/forces_rmse_metrics and train_metric_step/forces_rmse_summary\n",
      "Column 'val1_epoch/per_atom_energy_mae': Values differ -> keeping both as val1_epoch/per_atom_energy_mae_metrics and val1_epoch/per_atom_energy_mae_summary\n",
      "Column 'val1_epoch/per_atom_energy_rmse': Values differ -> keeping both as val1_epoch/per_atom_energy_rmse_metrics and val1_epoch/per_atom_energy_rmse_summary\n",
      "Column 'loss_coeff/forces_angle_forces_angle': Values differ -> keeping both as loss_coeff/forces_angle_forces_angle_metrics and loss_coeff/forces_angle_forces_angle_summary\n",
      "Column 'loss_coeff/forces_tail_huber': Values differ -> keeping both as loss_coeff/forces_tail_huber_metrics and loss_coeff/forces_tail_huber_summary\n",
      "Column 'val0_epoch/total_energy_rmse': Values differ -> keeping both as val0_epoch/total_energy_rmse_metrics and val0_epoch/total_energy_rmse_summary\n",
      "Column 'train_loss_step/stress_shear_stress_shear_mae': Values differ -> keeping both as train_loss_step/stress_shear_stress_shear_mae_metrics and train_loss_step/stress_shear_stress_shear_mae_summary\n",
      "Column 'train_loss_epoch/peratom_E_Huber': Values differ -> keeping both as train_loss_epoch/peratom_E_Huber_metrics and train_loss_epoch/peratom_E_Huber_summary\n",
      "Column 'cfg_lr_scheduler': Values differ -> keeping both as cfg_lr_scheduler_metrics and cfg_lr_scheduler_summary\n",
      "Column 'train_loss_epoch/weighted_sum': Values differ -> keeping both as train_loss_epoch/weighted_sum_metrics and train_loss_epoch/weighted_sum_summary\n",
      "Column 'val0_epoch/stress_rmse': Values differ -> keeping both as val0_epoch/stress_rmse_metrics and val0_epoch/stress_rmse_summary\n",
      "Column 'train_loss_step/forces_auto_stratified_huber': Values differ -> keeping both as train_loss_step/forces_auto_stratified_huber_metrics and train_loss_step/forces_auto_stratified_huber_summary\n",
      "Column 'loss_coeff/forces_mse': Values differ -> keeping both as loss_coeff/forces_mse_metrics and loss_coeff/forces_mse_summary\n",
      "Column 'train_metric_epoch/stress_mae': Values differ -> keeping both as train_metric_epoch/stress_mae_metrics and train_metric_epoch/stress_mae_summary\n",
      "Column 'train_loss_epoch/forces_angle_force_angle': Values differ -> keeping both as train_loss_epoch/forces_angle_force_angle_metrics and train_loss_epoch/forces_angle_force_angle_summary\n",
      "Column 'test0_epoch/weighted_sum': Values differ -> keeping both as test0_epoch/weighted_sum_metrics and test0_epoch/weighted_sum_summary\n",
      "Column 'train_loss_step/forces_angle_force_angle': Values differ -> keeping both as train_loss_step/forces_angle_force_angle_metrics and train_loss_step/forces_angle_force_angle_summary\n",
      "\n",
      "Merge completed:\n",
      "  - Columns kept from metrics only: 3\n",
      "  - Columns added from summary only: 1\n",
      "  - Columns with different values (kept both): 115\n",
      "  - Columns with identical values (kept metrics version): 4\n",
      "  - Total columns in result: 238\n",
      "\n",
      "Final merged dataframe shape: (91, 238)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3· SMART MERGE STRATEGY\n",
    "# -----------------------------------------------------------\n",
    "def smart_merge_with_comparison(metrics_df, summary_df, join_key='run'):\n",
    "    \"\"\"\n",
    "    Merge two dataframes intelligently:\n",
    "    - Keep unique columns from both\n",
    "    - For overlapping columns, check if values differ\n",
    "    - If they differ, keep both with _metrics and _summary suffixes\n",
    "    - If they're the same, keep just one copy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get overlapping columns (excluding join key)\n",
    "    overlapping_cols = [col for col in set(metrics_df.columns) & set(summary_df.columns) if col != join_key]\n",
    "    \n",
    "    # Start with metrics dataframe\n",
    "    result = metrics_df.copy()\n",
    "    \n",
    "    # Get unique columns from summary\n",
    "    summary_unique = [col for col in summary_df.columns if col not in metrics_df.columns]\n",
    "    \n",
    "    # Add unique summary columns\n",
    "    if summary_unique:\n",
    "        summary_unique_df = summary_df[[join_key] + summary_unique]\n",
    "        result = result.merge(summary_unique_df, on=join_key, how='left')\n",
    "    \n",
    "    # Handle overlapping columns\n",
    "    cols_to_keep_both = []\n",
    "    cols_identical = []\n",
    "    \n",
    "    for col in overlapping_cols:\n",
    "        # Sample comparison on a subset of rows\n",
    "        sample_size = min(1000, len(metrics_df))\n",
    "        sample_metrics = metrics_df.sample(sample_size, random_state=42)[[join_key, col]]\n",
    "        sample_summary = summary_df[[join_key, col]]\n",
    "        \n",
    "        # Merge for comparison\n",
    "        comparison = sample_metrics.merge(sample_summary, on=join_key, how='inner', suffixes=('_m', '_s'))\n",
    "        \n",
    "        if len(comparison) > 0:\n",
    "            try:\n",
    "                # Check if values are different\n",
    "                different_mask = comparison[col + '_m'] != comparison[col + '_s']\n",
    "                if different_mask.any():\n",
    "                    cols_to_keep_both.append(col)\n",
    "                    print(f\"Column '{col}': Values differ -> keeping both as {col}_metrics and {col}_summary\")\n",
    "                else:\n",
    "                    cols_identical.append(col)\n",
    "                    print(f\"Column '{col}': Values identical -> keeping metrics version only\")\n",
    "            except Exception as e:\n",
    "                # If comparison fails (e.g., different types), keep both to be safe\n",
    "                cols_to_keep_both.append(col)\n",
    "                print(f\"Column '{col}': Could not compare -> keeping both to be safe\")\n",
    "    \n",
    "    # Add columns where values differ (with suffixes)\n",
    "    if cols_to_keep_both:\n",
    "        summary_different = summary_df[[join_key] + cols_to_keep_both]\n",
    "        result = result.merge(summary_different, on=join_key, how='left', suffixes=('_metrics', '_summary'))\n",
    "    \n",
    "    print(f\"\\nMerge completed:\")\n",
    "    print(f\"  - Columns kept from metrics only: {len(metrics_df.columns) - len(overlapping_cols)}\")\n",
    "    print(f\"  - Columns added from summary only: {len(summary_unique)}\")\n",
    "    print(f\"  - Columns with different values (kept both): {len(cols_to_keep_both)}\")\n",
    "    print(f\"  - Columns with identical values (kept metrics version): {len(cols_identical)}\")\n",
    "    print(f\"  - Total columns in result: {len(result.columns)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply smart merge\n",
    "if summary is not None:\n",
    "    print(\"=== APPLYING SMART MERGE ===\")\n",
    "    merged_data = smart_merge_with_comparison(metrics, summary)\n",
    "    print(f\"\\nFinal merged dataframe shape: {merged_data.shape}\")\n",
    "else:\n",
    "    merged_data = metrics.copy()\n",
    "    print(\"No summary data available, using metrics only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Enhanced Configuration Parsing\n",
    "\n",
    "This section extracts comprehensive information from the training configurations:\n",
    "\n",
    "### Model Hyperparameters (`cfg_model`)\n",
    "- Architecture: `r_max`, `num_layers`, `l_max`, `mlp_depth`, `mlp_width`\n",
    "- Features: `num_scalar_features`, `num_tensor_features`, `radial_embed_dim`\n",
    "- Advanced: `parity`, `tp_path_channel_coupling`, `bessel_trainable`\n",
    "- Cutoff: `poly_p`, `num_bessels` from nested configurations\n",
    "\n",
    "### Training Setup (`cfg_info_dict`)\n",
    "- **Dataset Tracking**: File paths + MD5 hash for dataset comparison\n",
    "- **Loss Functions**: Complete loss configuration with coefficients  \n",
    "- **Callbacks**: ModelCheckpoint, LearningRateMonitor, etc.\n",
    "- **Scheduler**: Learning rate scheduling details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL DATASET OVERVIEW ===\n",
      "Shape: (91, 238)\n",
      "Columns: 238\n",
      "\n",
      "Columns with suffixes (indicating differences were found):\n",
      "  project_metrics\n",
      "  _runtime_metrics\n",
      "  _step_metrics\n",
      "  _timestamp_metrics\n",
      "  _wandb_metrics\n",
      "  epoch_metrics\n",
      "  loss_coeff/forces_tail_huber_metrics\n",
      "  loss_coeff/per_atom_energy_mse_metrics\n",
      "  loss_coeff/stress_mse_metrics\n",
      "  lr-Adam_metrics\n",
      "  test0_epoch/forces_mae_metrics\n",
      "  test0_epoch/forces_rmse_metrics\n",
      "  test0_epoch/per_atom_energy_mae_metrics\n",
      "  test0_epoch/per_atom_energy_rmse_metrics\n",
      "  test0_epoch/stress_mae_metrics\n",
      "  test0_epoch/stress_rmse_metrics\n",
      "  test0_epoch/weighted_sum_metrics\n",
      "  train_loss_epoch/forces_tail_huber_metrics\n",
      "  train_loss_epoch/per_atom_energy_mse_metrics\n",
      "  train_loss_epoch/stress_mse_metrics\n",
      "  train_loss_epoch/weighted_sum_metrics\n",
      "  train_loss_step/forces_tail_huber_metrics\n",
      "  train_loss_step/per_atom_energy_mse_metrics\n",
      "  train_loss_step/stress_mse_metrics\n",
      "  train_loss_step/weighted_sum_metrics\n",
      "  trainer/global_step_metrics\n",
      "  val0_epoch/forces_rmse_metrics\n",
      "  val0_epoch/per_atom_energy_mae_metrics\n",
      "  val0_epoch/per_atom_energy_rmse_metrics\n",
      "  val0_epoch/stress_mae_metrics\n",
      "  val0_epoch/stress_rmse_metrics\n",
      "  val0_epoch/weighted_sum_metrics\n",
      "  val1_epoch/forces_mae_metrics\n",
      "  val1_epoch/forces_rmse_metrics\n",
      "  val1_epoch/per_atom_energy_mae_metrics\n",
      "  val1_epoch/per_atom_energy_rmse_metrics\n",
      "  val1_epoch/stress_mae_metrics\n",
      "  val1_epoch/stress_rmse_metrics\n",
      "  val1_epoch/weighted_sum_metrics\n",
      "  cfg_info_dict_metrics\n",
      "  cfg_loss_metrics\n",
      "  cfg_lr_scheduler_metrics\n",
      "  cfg_model_metrics\n",
      "  cfg_num_datasets_metrics\n",
      "  cfg_test_metrics\n",
      "  cfg_train_metrics_metrics\n",
      "  cfg_val_metrics\n",
      "  train_loss/auto_delta_metrics\n",
      "  loss_coeff/forces_auto_stratified_huber_metrics\n",
      "  train_loss_epoch/forces_auto_stratified_huber_metrics\n",
      "  train_loss_step/forces_auto_stratified_huber_metrics\n",
      "  loss_coeff/forces_focal_focal_mse_metrics\n",
      "  train_loss_epoch/forces_focal_focal_mse_metrics\n",
      "  train_loss_step/forces_focal_focal_mse_metrics\n",
      "  loss_coeff/forces_angle_forces_angle_metrics\n",
      "  loss_coeff/stress_shear_stress_shear_mae_metrics\n",
      "  train_loss_epoch/forces_angle_forces_angle_metrics\n",
      "  train_loss_epoch/stress_shear_stress_shear_mae_metrics\n",
      "  train_loss_step/forces_angle_forces_angle_metrics\n",
      "  train_loss_step/stress_shear_stress_shear_mae_metrics\n",
      "  loss_coeff/forces_angle_force_angle_metrics\n",
      "  train_loss_epoch/forces_angle_force_angle_metrics\n",
      "  train_loss_step/forces_angle_force_angle_metrics\n",
      "  loss_coeff/stress_angle_stress_angle_metrics\n",
      "  train_loss_epoch/stress_angle_stress_angle_metrics\n",
      "  train_loss_step/stress_angle_stress_angle_metrics\n",
      "  loss_coeff/force_strat_E_huber_metrics\n",
      "  loss_coeff/peratom_E_Huber_metrics\n",
      "  loss_coeff/stress_E_Huber_metrics\n",
      "  lr-AdamW_metrics\n",
      "  test0_epoch/total_energy_mae_metrics\n",
      "  test0_epoch/total_energy_rmse_metrics\n",
      "  train_loss_epoch/force_strat_E_huber_metrics\n",
      "  train_loss_epoch/peratom_E_Huber_metrics\n",
      "  train_loss_epoch/stress_E_Huber_metrics\n",
      "  train_loss_step/force_strat_E_huber_metrics\n",
      "  train_loss_step/peratom_E_Huber_metrics\n",
      "  train_loss_step/stress_E_Huber_metrics\n",
      "  train_metric_epoch/forces_mae_metrics\n",
      "  train_metric_epoch/forces_rmse_metrics\n",
      "  train_metric_epoch/per_atom_energy_mae_metrics\n",
      "  train_metric_epoch/per_atom_energy_rmse_metrics\n",
      "  train_metric_epoch/stress_mae_metrics\n",
      "  train_metric_epoch/stress_rmse_metrics\n",
      "  train_metric_epoch/total_energy_mae_metrics\n",
      "  train_metric_epoch/total_energy_rmse_metrics\n",
      "  train_metric_epoch/weighted_sum_metrics\n",
      "  train_metric_step/forces_mae_metrics\n",
      "  train_metric_step/forces_rmse_metrics\n",
      "  train_metric_step/per_atom_energy_mae_metrics\n",
      "  train_metric_step/per_atom_energy_rmse_metrics\n",
      "  train_metric_step/stress_mae_metrics\n",
      "  train_metric_step/stress_rmse_metrics\n",
      "  train_metric_step/total_energy_mae_metrics\n",
      "  train_metric_step/total_energy_rmse_metrics\n",
      "  train_metric_step/weighted_sum_metrics\n",
      "  val0_epoch/total_energy_mae_metrics\n",
      "  val0_epoch/total_energy_rmse_metrics\n",
      "  loss_coeff/forces_stratified_huber_metrics\n",
      "  train_loss_epoch/forces_stratified_huber_metrics\n",
      "  train_loss_step/forces_stratified_huber_metrics\n",
      "  loss_coeff/forces_huber_metrics\n",
      "  loss_coeff/stress_huber_metrics\n",
      "  train_loss_epoch/forces_huber_metrics\n",
      "  train_loss_epoch/per_atom_energy_huber_metrics\n",
      "  train_loss_epoch/stress_huber_metrics\n",
      "  train_loss_step/forces_huber_metrics\n",
      "  train_loss_step/per_atom_energy_huber_metrics\n",
      "  train_loss_step/stress_huber_metrics\n",
      "  loss_coeff/forces_mse_metrics\n",
      "  train_loss_epoch/forces_mse_metrics\n",
      "  train_loss_step/forces_mse_metrics\n",
      "  loss_coeff/per_atom_energy_huber_metrics\n",
      "  cfg_gradient_clip_algorithm_metrics\n",
      "  cfg_gradient_clip_val_metrics\n",
      "  cfg_lsqr_metrics\n",
      "  cfg_norm_eps_metrics\n",
      "  loss_coeff/force_strat_E_huber_summary\n",
      "  train_metric_epoch/total_energy_mae_summary\n",
      "  test0_epoch/total_energy_rmse_summary\n",
      "  train_loss_step/forces_stratified_huber_summary\n",
      "  train_metric_epoch/per_atom_energy_rmse_summary\n",
      "  train_loss_step/forces_tail_huber_summary\n",
      "  train_loss_step/forces_angle_forces_angle_summary\n",
      "  loss_coeff/forces_angle_force_angle_summary\n",
      "  train_loss_step/weighted_sum_summary\n",
      "  train_loss_epoch/stress_E_Huber_summary\n",
      "  test0_epoch/per_atom_energy_mae_summary\n",
      "  train_loss/auto_delta_summary\n",
      "  trainer/global_step_summary\n",
      "  test0_epoch/forces_mae_summary\n",
      "  train_metric_step/stress_mae_summary\n",
      "  cfg_lsqr_summary\n",
      "  loss_coeff/forces_auto_stratified_huber_summary\n",
      "  train_loss_step/stress_mse_summary\n",
      "  train_loss_epoch/forces_focal_focal_mse_summary\n",
      "  train_metric_step/per_atom_energy_mae_summary\n",
      "  loss_coeff/stress_angle_stress_angle_summary\n",
      "  val0_epoch/per_atom_energy_mae_summary\n",
      "  train_metric_step/weighted_sum_summary\n",
      "  val0_epoch/weighted_sum_summary\n",
      "  cfg_train_metrics_summary\n",
      "  val0_epoch/per_atom_energy_rmse_summary\n",
      "  train_loss_epoch/forces_stratified_huber_summary\n",
      "  lr-AdamW_summary\n",
      "  cfg_loss_summary\n",
      "  epoch_summary\n",
      "  train_loss_step/stress_angle_stress_angle_summary\n",
      "  train_loss_epoch/forces_huber_summary\n",
      "  train_loss_epoch/stress_huber_summary\n",
      "  _runtime_summary\n",
      "  train_loss_epoch/per_atom_energy_huber_summary\n",
      "  train_metric_epoch/per_atom_energy_mae_summary\n",
      "  train_metric_step/forces_mae_summary\n",
      "  test0_epoch/total_energy_mae_summary\n",
      "  val1_epoch/weighted_sum_summary\n",
      "  train_loss_step/forces_focal_focal_mse_summary\n",
      "  train_loss_step/forces_huber_summary\n",
      "  val1_epoch/stress_rmse_summary\n",
      "  train_metric_step/total_energy_mae_summary\n",
      "  train_metric_step/total_energy_rmse_summary\n",
      "  lr-Adam_summary\n",
      "  val0_epoch/total_energy_mae_summary\n",
      "  train_loss_epoch/force_strat_E_huber_summary\n",
      "  train_loss_epoch/stress_shear_stress_shear_mae_summary\n",
      "  train_metric_epoch/total_energy_rmse_summary\n",
      "  train_loss_step/force_strat_E_huber_summary\n",
      "  train_loss_step/forces_mse_summary\n",
      "  _timestamp_summary\n",
      "  cfg_gradient_clip_val_summary\n",
      "  train_loss_step/per_atom_energy_huber_summary\n",
      "  cfg_info_dict_summary\n",
      "  project_summary\n",
      "  test0_epoch/forces_rmse_summary\n",
      "  train_metric_epoch/weighted_sum_summary\n",
      "  cfg_norm_eps_summary\n",
      "  train_loss_epoch/forces_angle_forces_angle_summary\n",
      "  train_metric_step/per_atom_energy_rmse_summary\n",
      "  train_loss_epoch/per_atom_energy_mse_summary\n",
      "  loss_coeff/stress_mse_summary\n",
      "  train_loss_step/stress_huber_summary\n",
      "  loss_coeff/forces_stratified_huber_summary\n",
      "  cfg_num_datasets_summary\n",
      "  train_loss_step/peratom_E_Huber_summary\n",
      "  train_metric_epoch/forces_rmse_summary\n",
      "  loss_coeff/per_atom_energy_huber_summary\n",
      "  train_metric_step/stress_rmse_summary\n",
      "  _step_summary\n",
      "  val1_epoch/stress_mae_summary\n",
      "  train_loss_epoch/forces_tail_huber_summary\n",
      "  val1_epoch/forces_rmse_summary\n",
      "  train_loss_epoch/stress_angle_stress_angle_summary\n",
      "  loss_coeff/forces_huber_summary\n",
      "  train_loss_epoch/stress_mse_summary\n",
      "  val1_epoch/forces_mae_summary\n",
      "  loss_coeff/peratom_E_Huber_summary\n",
      "  train_loss_step/per_atom_energy_mse_summary\n",
      "  test0_epoch/stress_mae_summary\n",
      "  train_loss_epoch/forces_auto_stratified_huber_summary\n",
      "  loss_coeff/stress_huber_summary\n",
      "  train_loss_epoch/forces_mse_summary\n",
      "  loss_coeff/per_atom_energy_mse_summary\n",
      "  val0_epoch/stress_mae_summary\n",
      "  cfg_model_summary\n",
      "  _wandb_summary\n",
      "  test0_epoch/per_atom_energy_rmse_summary\n",
      "  train_metric_epoch/stress_rmse_summary\n",
      "  test0_epoch/stress_rmse_summary\n",
      "  cfg_gradient_clip_algorithm_summary\n",
      "  train_metric_epoch/forces_mae_summary\n",
      "  val0_epoch/forces_rmse_summary\n",
      "  loss_coeff/stress_E_Huber_summary\n",
      "  loss_coeff/stress_shear_stress_shear_mae_summary\n",
      "  train_loss_step/stress_E_Huber_summary\n",
      "  loss_coeff/forces_focal_focal_mse_summary\n",
      "  train_metric_step/forces_rmse_summary\n",
      "  val1_epoch/per_atom_energy_mae_summary\n",
      "  val1_epoch/per_atom_energy_rmse_summary\n",
      "  loss_coeff/forces_angle_forces_angle_summary\n",
      "  loss_coeff/forces_tail_huber_summary\n",
      "  val0_epoch/total_energy_rmse_summary\n",
      "  train_loss_step/stress_shear_stress_shear_mae_summary\n",
      "  train_loss_epoch/peratom_E_Huber_summary\n",
      "  cfg_lr_scheduler_summary\n",
      "  train_loss_epoch/weighted_sum_summary\n",
      "  val0_epoch/stress_rmse_summary\n",
      "  train_loss_step/forces_auto_stratified_huber_summary\n",
      "  loss_coeff/forces_mse_summary\n",
      "  train_metric_epoch/stress_mae_summary\n",
      "  train_loss_epoch/forces_angle_force_angle_summary\n",
      "  test0_epoch/weighted_sum_summary\n",
      "  train_loss_step/forces_angle_force_angle_summary\n",
      "\n",
      "Validation metrics available:\n",
      "  val0_epoch/forces_mae_x\n",
      "  val0_epoch/forces_rmse_metrics\n",
      "  val0_epoch/stress_mae_metrics\n",
      "  val0_epoch/stress_rmse_metrics\n",
      "  val0_epoch/forces_mae_y\n",
      "  val0_epoch/forces_mae\n",
      "  val0_epoch/stress_mae_summary\n",
      "  val0_epoch/forces_rmse_summary\n",
      "  val0_epoch/stress_rmse_summary\n",
      "\n",
      "============================================================\n",
      "✅ Data exploration and merging complete!\n",
      "✅ You can now proceed with your analysis using 'merged_data'\n",
      "✅ No data loss - differences preserved with suffixes when found\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4· PROCEED WITH ANALYSIS USING MERGED DATA\n",
    "# -----------------------------------------------------------\n",
    "# Now use 'merged_data' instead of 'metrics' for your analysis\n",
    "\n",
    "# Show column overview\n",
    "print(\"=== FINAL DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {len(merged_data.columns)}\")\n",
    "\n",
    "# Show any suffixed columns\n",
    "suffixed_cols = [col for col in merged_data.columns if col.endswith(('_metrics', '_summary'))]\n",
    "if suffixed_cols:\n",
    "    print(f\"\\nColumns with suffixes (indicating differences were found):\")\n",
    "    for col in suffixed_cols:\n",
    "        print(f\"  {col}\")\n",
    "\n",
    "# Create run UID for analysis\n",
    "merged_data[\"run_uid\"] = merged_data[\"project_metrics\"].astype(str) + \"/\" + merged_data[\"run\"].astype(str)\n",
    "\n",
    "# Check what validation metrics we have available\n",
    "val_metrics = [col for col in merged_data.columns if 'val0_epoch' in col and ('forces' in col or 'stress' in col)]\n",
    "print(f\"\\nValidation metrics available:\")\n",
    "for col in val_metrics[:10]:  # Show first 10\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Data exploration and merging complete!\")\n",
    "print(\"✅ You can now proceed with your analysis using 'merged_data'\")\n",
    "print(\"✅ No data loss - differences preserved with suffixes when found\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# CONFIGURATION PARSING FUNCTIONS\n",
    "# -----------------------------------------------------------\n",
    "import hashlib\n",
    "\n",
    "def parse_cfg_model(cell: str) -> dict:\n",
    "    \"\"\"Extract hyperparameters from cfg_model JSON.\"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(cell, str):\n",
    "        return out\n",
    "    \n",
    "    try:\n",
    "        # Parse as JSON\n",
    "        cfg = json.loads(cell)\n",
    "        \n",
    "        # Direct field extraction from cfg_model\n",
    "        if \"r_max\" in cfg:\n",
    "            out[\"r_max\"] = float(cfg[\"r_max\"])\n",
    "        if \"num_layers\" in cfg:\n",
    "            out[\"num_layers\"] = int(cfg[\"num_layers\"])\n",
    "        if \"l_max\" in cfg:\n",
    "            out[\"l_max\"] = int(cfg[\"l_max\"])\n",
    "        if \"model_dtype\" in cfg:\n",
    "            out[\"model_dtype\"] = cfg[\"model_dtype\"]\n",
    "        if \"seed\" in cfg:\n",
    "            out[\"seed\"] = int(cfg[\"seed\"])\n",
    "        if \"avg_num_neighbors\" in cfg:\n",
    "            out[\"avg_num_neighbors\"] = float(cfg[\"avg_num_neighbors\"])\n",
    "        if \"num_scalar_features\" in cfg:\n",
    "            out[\"num_scalar_features\"] = int(cfg[\"num_scalar_features\"])\n",
    "        if \"num_tensor_features\" in cfg:\n",
    "            out[\"num_tensor_features\"] = int(cfg[\"num_tensor_features\"])\n",
    "        if \"allegro_mlp_hidden_layers_depth\" in cfg:\n",
    "            out[\"mlp_depth\"] = int(cfg[\"allegro_mlp_hidden_layers_depth\"])\n",
    "        if \"allegro_mlp_hidden_layers_width\" in cfg:\n",
    "            out[\"mlp_width\"] = int(cfg[\"allegro_mlp_hidden_layers_width\"])\n",
    "        if \"scalar_embed_mlp_hidden_layers_depth\" in cfg:\n",
    "            out[\"scalar_mlp_depth\"] = int(cfg[\"scalar_embed_mlp_hidden_layers_depth\"])\n",
    "        if \"scalar_embed_mlp_hidden_layers_width\" in cfg:\n",
    "            out[\"scalar_mlp_width\"] = int(cfg[\"scalar_embed_mlp_hidden_layers_width\"])\n",
    "        if \"radial_chemical_embed_dim\" in cfg:\n",
    "            out[\"radial_embed_dim\"] = int(cfg[\"radial_chemical_embed_dim\"])\n",
    "        if \"parity\" in cfg:\n",
    "            out[\"parity\"] = bool(cfg[\"parity\"])\n",
    "        if \"per_type_energy_scales_trainable\" in cfg:\n",
    "            out[\"per_type_energy_scales_trainable\"] = bool(cfg[\"per_type_energy_scales_trainable\"])\n",
    "        if \"per_type_energy_shifts_trainable\" in cfg:\n",
    "            out[\"per_type_energy_shifts_trainable\"] = bool(cfg[\"per_type_energy_shifts_trainable\"])\n",
    "        if \"tp_path_channel_coupling\" in cfg:\n",
    "            out[\"tp_path_channel_coupling\"] = bool(cfg[\"tp_path_channel_coupling\"])\n",
    "        if \"scalar_embed_mlp_nonlinearity\" in cfg:\n",
    "            out[\"scalar_embed_mlp_nonlinearity\"] = cfg[\"scalar_embed_mlp_nonlinearity\"]\n",
    "        \n",
    "        # Extract polynomial_cutoff_p from nested radial_chemical_embed\n",
    "        if \"radial_chemical_embed\" in cfg and isinstance(cfg[\"radial_chemical_embed\"], dict):\n",
    "            if \"polynomial_cutoff_p\" in cfg[\"radial_chemical_embed\"]:\n",
    "                out[\"poly_p\"] = int(cfg[\"radial_chemical_embed\"][\"polynomial_cutoff_p\"])\n",
    "            if \"num_bessels\" in cfg[\"radial_chemical_embed\"]:\n",
    "                out[\"num_bessels\"] = int(cfg[\"radial_chemical_embed\"][\"num_bessels\"])\n",
    "            if \"bessel_trainable\" in cfg[\"radial_chemical_embed\"]:\n",
    "                out[\"bessel_trainable\"] = bool(cfg[\"radial_chemical_embed\"][\"bessel_trainable\"])\n",
    "                \n",
    "    except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:\n",
    "        # Fallback to regex if JSON parsing fails\n",
    "        if m := re.search(r\"\\br_max\\s+([0-9]*\\.?[0-9]+)\", cell):\n",
    "            out[\"r_max\"] = float(m.group(1))\n",
    "        if m := re.search(r\"polynomial_cutoff_p\\s+([0-9]+)\", cell):\n",
    "            out[\"poly_p\"] = int(m.group(1))\n",
    "        if m := re.search(r\"\\bnum_layers\\s+([0-9]+)\", cell):\n",
    "            out[\"num_layers\"] = int(m.group(1))\n",
    "    \n",
    "    return out\n",
    "\n",
    "def parse_cfg_info_dict(cell: str) -> dict:\n",
    "    \"\"\"Extract training info from cfg_info_dict JSON including dataset paths, loss function, and callbacks.\"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(cell, str):\n",
    "        return out\n",
    "    \n",
    "    try:\n",
    "        # Parse as JSON\n",
    "        cfg = json.loads(cell)\n",
    "        \n",
    "        # Extract dataset file paths\n",
    "        if \"data\" in cfg:\n",
    "            data_cfg = cfg[\"data\"]\n",
    "            if \"train_file_path\" in data_cfg:\n",
    "                out[\"train_file_path\"] = data_cfg[\"train_file_path\"]\n",
    "            if \"val_file_path\" in data_cfg:\n",
    "                val_paths = data_cfg[\"val_file_path\"]\n",
    "                if isinstance(val_paths, list):\n",
    "                    out[\"val_file_path\"] = \"|\".join(sorted(val_paths))  # Sort for consistent hashing\n",
    "                else:\n",
    "                    out[\"val_file_path\"] = val_paths\n",
    "            if \"test_file_path\" in data_cfg:\n",
    "                out[\"test_file_path\"] = data_cfg[\"test_file_path\"]\n",
    "            \n",
    "            # Create dataset hash for comparison\n",
    "            dataset_components = []\n",
    "            if \"train_file_path\" in out:\n",
    "                dataset_components.append(f\"train:{out['train_file_path']}\")\n",
    "            if \"val_file_path\" in out:\n",
    "                dataset_components.append(f\"val:{out['val_file_path']}\")\n",
    "            if \"test_file_path\" in out:\n",
    "                dataset_components.append(f\"test:{out['test_file_path']}\")\n",
    "            \n",
    "            if dataset_components:\n",
    "                dataset_string = \"|\".join(dataset_components)\n",
    "                out[\"dataset_hash\"] = hashlib.md5(dataset_string.encode()).hexdigest()[:8]\n",
    "                out[\"dataset_signature\"] = dataset_string\n",
    "        \n",
    "        # Extract loss function details\n",
    "        if \"training_module\" in cfg and \"loss\" in cfg[\"training_module\"]:\n",
    "            loss_cfg = cfg[\"training_module\"][\"loss\"]\n",
    "            if \"metrics\" in loss_cfg:\n",
    "                loss_metrics = []\n",
    "                for metric in loss_cfg[\"metrics\"]:\n",
    "                    if \"name\" in metric:\n",
    "                        loss_info = f\"{metric['name']}\"\n",
    "                        if \"coeff\" in metric:\n",
    "                            loss_info += f\"(coeff={metric['coeff']})\"\n",
    "                        loss_metrics.append(loss_info)\n",
    "                out[\"loss_functions\"] = \"|\".join(loss_metrics)\n",
    "        \n",
    "        # Extract callback information\n",
    "        if \"trainer\" in cfg and \"callbacks\" in cfg[\"trainer\"]:\n",
    "            callbacks = cfg[\"trainer\"][\"callbacks\"]\n",
    "            callback_types = []\n",
    "            for callback in callbacks:\n",
    "                if \"_target_\" in callback:\n",
    "                    callback_type = callback[\"_target_\"].split(\".\")[-1]  # Get class name\n",
    "                    callback_types.append(callback_type)\n",
    "                    \n",
    "                    # Extract specific callback details\n",
    "                    if \"ModelCheckpoint\" in callback_type:\n",
    "                        if \"monitor\" in callback:\n",
    "                            out[\"checkpoint_monitor\"] = callback[\"monitor\"]\n",
    "                        if \"mode\" in callback:\n",
    "                            out[\"checkpoint_mode\"] = callback[\"mode\"]\n",
    "                        if \"save_top_k\" in callback:\n",
    "                            out[\"checkpoint_save_top_k\"] = callback[\"save_top_k\"]\n",
    "            \n",
    "            out[\"callback_types\"] = \"|\".join(callback_types)\n",
    "        \n",
    "        # Extract scheduler information  \n",
    "        if \"training_module\" in cfg and \"lr_scheduler\" in cfg[\"training_module\"]:\n",
    "            scheduler_cfg = cfg[\"training_module\"][\"lr_scheduler\"]\n",
    "            if \"scheduler\" in scheduler_cfg and \"_target_\" in scheduler_cfg[\"scheduler\"]:\n",
    "                out[\"lr_scheduler_type\"] = scheduler_cfg[\"scheduler\"][\"_target_\"].split(\".\")[-1]\n",
    "            if \"monitor\" in scheduler_cfg:\n",
    "                out[\"lr_scheduler_monitor\"] = scheduler_cfg[\"monitor\"]\n",
    "            if \"frequency\" in scheduler_cfg:\n",
    "                out[\"lr_scheduler_frequency\"] = scheduler_cfg[\"frequency\"]\n",
    "                \n",
    "    except (json.JSONDecodeError, KeyError, ValueError, TypeError) as e:\n",
    "        # Silently fail for missing or malformed cfg_info_dict\n",
    "        pass\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Analysis-Ready Dataset\n",
    "\n",
    "Apply all parsing functions and create the final enhanced dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config column: cfg_model_metrics\n",
      "Using info column: cfg_info_dict_metrics\n",
      "=== PARSED HYPERPARAMETERS & TRAINING INFO ===\n",
      "\n",
      "📊 Model Hyperparameters (21 fields):\n",
      "  r_max: 5.0\n",
      "  num_layers: 2.0\n",
      "  l_max: 2.0\n",
      "  model_dtype: float32\n",
      "  seed: 42.0\n",
      "  avg_num_neighbors: 32.03126333894399\n",
      "  num_scalar_features: 128.0\n",
      "  num_tensor_features: 64.0\n",
      "  mlp_depth: 2.0\n",
      "  mlp_width: 512.0\n",
      "  scalar_mlp_depth: 2.0\n",
      "  scalar_mlp_width: 256.0\n",
      "  radial_embed_dim: 128.0\n",
      "  parity: False\n",
      "  per_type_energy_scales_trainable: False\n",
      "  per_type_energy_shifts_trainable: False\n",
      "  tp_path_channel_coupling: True\n",
      "  scalar_embed_mlp_nonlinearity: silu\n",
      "  poly_p: 6.0\n",
      "  num_bessels: 8.0\n",
      "  bessel_trainable: False\n",
      "\n",
      "🔧 Training Configuration (13 fields):\n",
      "  train_file_path: data/base_huber_q0.9_no_sampler_train.xyz\n",
      "  val_file_path: data/base_huber_q0.9_no_sampler_val.xyz|data/base_huber_q0.9_no_sampler_val_b...\n",
      "  test_file_path: data/base_huber_q0.9_no_sampler_test.xyz\n",
      "  dataset_hash: 053fe93a\n",
      "  dataset_signature: train:data/base_huber_q0.9_no_sampler_train.xyz|val:data/base_huber_q0.9_no_s...\n",
      "  loss_functions: per_atom_energy_mse(coeff=1)|stress_mse(coeff=50)|forces_tail_huber(coeff=10)\n",
      "  checkpoint_monitor: val1_epoch/forces_mae\n",
      "  checkpoint_mode: min\n",
      "  checkpoint_save_top_k: 5.0\n",
      "  callback_types: ModelCheckpoint|LearningRateMonitor|LossCoefficientMonitor\n",
      "  lr_scheduler_type: ReduceLROnPlateau\n",
      "  lr_scheduler_monitor: val0_epoch/weighted_sum\n",
      "  lr_scheduler_frequency: 1.0\n",
      "\n",
      "📁 Dataset Comparison Examples:\n",
      "Found 41 unique dataset combinations:\n",
      "  Hash 0e9c7bd3: 28 runs\n",
      "    └─ train:data/job_gen_7-2025-04-29_train.xyz|val:data/job_gen_7-2025-04-29_val.xyz|test:data/job_gen_7-2025-04-29_test.xyz\n",
      "  Hash d6bea18f: 9 runs\n",
      "    └─ train:data/base_huber_q0.75_rare_sampling_train.xyz|val:data/base_huber_q0.75_rare_sampling_val.xyz|test:data/base_huber_q0.75_rare_sampling_test.xyz\n",
      "  Hash b465f3b4: 4 runs\n",
      "    └─ train:data/huber_q0.85_rare_sampling_train.xyz|val:data/huber_q0.85_rare_sampling_val.xyz|data/huber_q0.85_rare_sampling_val_b.xyz|test:data/huber_q0.85_rare_sampling_test.xyz\n",
      "    ... and 38 more combinations\n",
      "\n",
      "🎯 Loss Function Analysis:\n",
      "Found 19 unique loss function combinations:\n",
      "  per_atom_energy_mse(coeff=1)|forces_stratified_huber(coeff=10)|stress_mse(coeff=100): 23 runs\n",
      "  per_atom_energy_mse(coeff=1)|stress_mse(coeff=50)|forces_tail_huber(coeff=10): 20 runs\n",
      "  per_atom_energy_mse(coeff=1)|stress_mse(coeff=1)|forces_tail_huber(coeff=10): 10 runs\n",
      "\n",
      "============================================================\n",
      "✅ Enhanced parsing complete! Final dataset shape: (91, 275)\n",
      "✅ Available hyperparameters: ['r_max', 'num_layers', 'l_max', 'model_dtype', 'seed', 'avg_num_neighbors', 'num_scalar_features', 'num_tensor_features', 'mlp_depth', 'mlp_width', 'scalar_mlp_depth', 'scalar_mlp_width', 'radial_embed_dim', 'parity', 'per_type_energy_scales_trainable', 'per_type_energy_shifts_trainable', 'tp_path_channel_coupling', 'scalar_embed_mlp_nonlinearity', 'poly_p', 'num_bessels', 'bessel_trainable']\n",
      "✅ Available training info: ['train_file_path', 'val_file_path', 'test_file_path', 'dataset_hash', 'dataset_signature', 'loss_functions', 'checkpoint_monitor', 'checkpoint_mode', 'checkpoint_save_top_k', 'callback_types', 'lr_scheduler_type', 'lr_scheduler_monitor', 'lr_scheduler_frequency']\n",
      "✅ Dataset hashing enables easy model comparison\n",
      "✅ Loss function details extracted for analysis\n",
      "✅ Callback and scheduler info available\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# APPLY PARSING & CREATE ANALYSIS-READY DATASET\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Apply parsing to the correct columns (handle suffixes if they exist)\n",
    "# Parse cfg_model\n",
    "cfg_col = 'cfg_model'\n",
    "if 'cfg_model_metrics' in merged_data.columns:\n",
    "    cfg_col = 'cfg_model_metrics'  # Use metrics version if both exist\n",
    "elif 'cfg_model_summary' in merged_data.columns:\n",
    "    cfg_col = 'cfg_model_summary'\n",
    "\n",
    "print(f\"Using config column: {cfg_col}\")\n",
    "cfg_parsed = merged_data[cfg_col].apply(parse_cfg_model).apply(pd.Series)\n",
    "\n",
    "# Parse cfg_info_dict for dataset and training details\n",
    "info_col = 'cfg_info_dict'\n",
    "if 'cfg_info_dict_metrics' in merged_data.columns:\n",
    "    info_col = 'cfg_info_dict_metrics'\n",
    "elif 'cfg_info_dict_summary' in merged_data.columns:\n",
    "    info_col = 'cfg_info_dict_summary'\n",
    "\n",
    "print(f\"Using info column: {info_col}\")\n",
    "info_parsed = merged_data[info_col].apply(parse_cfg_info_dict).apply(pd.Series)\n",
    "\n",
    "# Combine all parsed data\n",
    "merged_data = pd.concat([merged_data, cfg_parsed, info_parsed], axis=1)\n",
    "\n",
    "# Create epoch helpers for analysis\n",
    "last_epochs = merged_data.groupby(\"run_uid\")[\"epoch_metrics\"].transform(\"max\")\n",
    "best_epochs = (\n",
    "    merged_data\n",
    "      .groupby(\"run_uid\")[\"val0_epoch/forces_rmse_metrics\"]\n",
    "      .transform(\"idxmin\")\n",
    "      .map(merged_data[\"epoch_metrics\"])\n",
    ")\n",
    "merged_data[\"epoch_last\"] = last_epochs\n",
    "merged_data[\"epoch_best\"] = best_epochs\n",
    "\n",
    "print(\"=== PARSED HYPERPARAMETERS & TRAINING INFO ===\")\n",
    "\n",
    "# Show sample of parsed model hyperparameters\n",
    "if len(cfg_parsed.columns) > 0:\n",
    "    print(f\"\\n📊 Model Hyperparameters ({len(cfg_parsed.columns)} fields):\")\n",
    "    sample_idx = 0\n",
    "    for col in cfg_parsed.columns:\n",
    "        value = cfg_parsed.iloc[sample_idx][col]\n",
    "        print(f\"  {col}: {value}\")\n",
    "\n",
    "# Show sample of parsed training info\n",
    "if len(info_parsed.columns) > 0:\n",
    "    print(f\"\\n🔧 Training Configuration ({len(info_parsed.columns)} fields):\")\n",
    "    sample_idx = 0\n",
    "    for col in info_parsed.columns:\n",
    "        value = info_parsed.iloc[sample_idx][col]\n",
    "        if isinstance(value, str) and len(str(value)) > 80:\n",
    "            print(f\"  {col}: {str(value)[:77]}...\")\n",
    "        else:\n",
    "            print(f\"  {col}: {value}\")\n",
    "\n",
    "# Show dataset hash functionality\n",
    "print(f\"\\n📁 Dataset Comparison Examples:\")\n",
    "if 'dataset_hash' in merged_data.columns:\n",
    "    dataset_groups = merged_data.groupby('dataset_hash').size().sort_values(ascending=False)\n",
    "    print(f\"Found {len(dataset_groups)} unique dataset combinations:\")\n",
    "    for i, (hash_val, count) in enumerate(dataset_groups.head(5).items()):\n",
    "        example_sig = merged_data[merged_data['dataset_hash'] == hash_val]['dataset_signature'].iloc[0]\n",
    "        print(f\"  Hash {hash_val}: {count} runs\")\n",
    "        print(f\"    └─ {example_sig}\")\n",
    "        if i >= 2:  # Show max 3 examples\n",
    "            break\n",
    "    \n",
    "    if len(dataset_groups) > 3:\n",
    "        print(f\"    ... and {len(dataset_groups) - 3} more combinations\")\n",
    "\n",
    "# Show loss function diversity\n",
    "print(f\"\\n🎯 Loss Function Analysis:\")\n",
    "if 'loss_functions' in merged_data.columns:\n",
    "    loss_groups = merged_data['loss_functions'].value_counts()\n",
    "    print(f\"Found {len(loss_groups)} unique loss function combinations:\")\n",
    "    for i, (loss_func, count) in enumerate(loss_groups.head(3).items()):\n",
    "        print(f\"  {loss_func}: {count} runs\")\n",
    "        if i >= 2:\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ Enhanced parsing complete! Final dataset shape: {merged_data.shape}\")\n",
    "print(f\"✅ Available hyperparameters: {list(cfg_parsed.columns)}\")\n",
    "print(f\"✅ Available training info: {list(info_parsed.columns)}\")\n",
    "print(\"✅ Dataset hashing enables easy model comparison\")\n",
    "print(\"✅ Loss function details extracted for analysis\")\n",
    "print(\"✅ Callback and scheduler info available\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Usage Examples\n",
    "\n",
    "Your dataset is now ready for advanced analysis! Here are some powerful ways to use the enhanced data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 ADVANCED ANALYSIS EXAMPLES\n",
      "==================================================\n",
      "\n",
      "📊 Example 1: Group by Dataset\n",
      "Found 41 unique datasets:\n",
      "  Dataset 0e9c7bd3: 28 models, best force RMSE: 0.1823\n",
      "  Dataset 3c50b7d5: 1 models, best force RMSE: 0.1938\n",
      "\n",
      "🔍 Example 2: Hyperparameter Clustering\n",
      "Found 5 hyperparameter combinations tested multiple times\n",
      "\n",
      "🎯 Example 3: Loss Function Impact\n",
      "Found 19 unique loss function configurations\n",
      "\n",
      "🏆 Example 4: Pareto Optimal Models\n",
      "Found 4 Pareto optimal models (best trade-offs)\n",
      "\n",
      "==================================================\n",
      "✅ Ready for your custom analysis!\n",
      "✅ Use merged_data for all further work\n",
      "✅ All configurations and metrics available\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# USAGE EXAMPLES - ADVANCED ANALYSIS PATTERNS\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"🔬 ADVANCED ANALYSIS EXAMPLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Group models by dataset to compare apples-to-apples\n",
    "print(\"\\n📊 Example 1: Group by Dataset\")\n",
    "if 'dataset_hash' in merged_data.columns:\n",
    "    dataset_analysis = (merged_data\n",
    "                       .groupby('dataset_hash')\n",
    "                       .agg(\n",
    "                           n_models=('run', 'count'),\n",
    "                           best_force_rmse=('val0_epoch/forces_rmse_metrics', 'min'),\n",
    "                           best_stress_rmse=('val0_epoch/stress_rmse_metrics', 'min'),\n",
    "                           dataset_name=('dataset_signature', 'first')\n",
    "                       )\n",
    "                       .sort_values('best_force_rmse'))\n",
    "    print(f\"Found {len(dataset_analysis)} unique datasets:\")\n",
    "    for i, (hash_val, row) in enumerate(dataset_analysis.head(3).iterrows()):\n",
    "        print(f\"  Dataset {hash_val}: {row['n_models']} models, best force RMSE: {row['best_force_rmse']:.4f}\")\n",
    "        if i >= 1:\n",
    "            break\n",
    "\n",
    "# 2. Find models with identical hyperparameters\n",
    "print(\"\\n🔍 Example 2: Hyperparameter Clustering\")\n",
    "if 'r_max' in merged_data.columns:\n",
    "    hp_groups = (merged_data\n",
    "                .groupby(['r_max', 'num_layers', 'poly_p'])\n",
    "                .agg(\n",
    "                    n_runs=('run', 'count'),\n",
    "                    datasets_used=('dataset_hash', 'nunique'),\n",
    "                    avg_force_rmse=('val0_epoch/forces_rmse_metrics', 'mean')\n",
    "                )\n",
    "                .query('n_runs > 1')  # Only groups with multiple runs\n",
    "                .sort_values('avg_force_rmse'))\n",
    "    print(f\"Found {len(hp_groups)} hyperparameter combinations tested multiple times\")\n",
    "\n",
    "# 3. Loss function analysis\n",
    "print(\"\\n🎯 Example 3: Loss Function Impact\")\n",
    "if 'loss_functions' in merged_data.columns:\n",
    "    loss_analysis = (merged_data\n",
    "                    .groupby('loss_functions')\n",
    "                    .agg(\n",
    "                        n_models=('run', 'count'),\n",
    "                        median_force_rmse=('val0_epoch/forces_rmse_metrics', 'median'),\n",
    "                        median_stress_rmse=('val0_epoch/stress_rmse_metrics', 'median')\n",
    "                    )\n",
    "                    .sort_values('median_force_rmse'))\n",
    "    print(f\"Found {len(loss_analysis)} unique loss function configurations\")\n",
    "\n",
    "# 4. Find the Pareto frontier\n",
    "print(\"\\n🏆 Example 4: Pareto Optimal Models\")\n",
    "force_rmse = merged_data['val0_epoch/forces_rmse_metrics']\n",
    "stress_rmse = merged_data['val0_epoch/stress_rmse_metrics']\n",
    "\n",
    "# Simple Pareto frontier identification\n",
    "pareto_mask = pd.Series(True, index=merged_data.index)\n",
    "for i in merged_data.index:\n",
    "    for j in merged_data.index:\n",
    "        if i != j:\n",
    "            if (force_rmse[j] <= force_rmse[i] and stress_rmse[j] <= stress_rmse[i] and \n",
    "                (force_rmse[j] < force_rmse[i] or stress_rmse[j] < stress_rmse[i])):\n",
    "                pareto_mask[i] = False\n",
    "                break\n",
    "\n",
    "pareto_models = merged_data[pareto_mask]\n",
    "print(f\"Found {len(pareto_models)} Pareto optimal models (best trade-offs)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Ready for your custom analysis!\")\n",
    "print(\"✅ Use merged_data for all further work\")\n",
    "print(\"✅ All configurations and metrics available\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ✅ Clean, Streamlined Workflow Complete!\n",
    "\n",
    "You now have a **robust, production-ready analysis pipeline** with:\n",
    "\n",
    "### 🔄 **Smart Data Integration**\n",
    "- Intelligent merging that preserves all data without loss\n",
    "- Automatic handling of overlapping columns with meaningful suffixes\n",
    "- No more column naming confusion or lost information\n",
    "\n",
    "### 🎯 **Comprehensive Config Parsing** \n",
    "- **Model hyperparameters**: All architecture settings extracted from JSON\n",
    "- **Dataset tracking**: MD5 hashes for easy model-dataset comparison  \n",
    "- **Training setup**: Loss functions, callbacks, schedulers fully parsed\n",
    "- **Analysis-ready**: All configs converted to clean, analyzable columns\n",
    "\n",
    "### 📊 **Enhanced Analysis Capabilities**\n",
    "- **Dataset comparison**: Group models by identical datasets\n",
    "- **Hyperparameter clustering**: Find optimal configuration patterns\n",
    "- **Loss function analysis**: Compare different training objectives\n",
    "- **Pareto optimization**: Identify best trade-off models\n",
    "\n",
    "### 🚀 **Next Steps**\n",
    "1. **Run cells 1-11** to get your analysis-ready dataset\n",
    "2. **Use `merged_data`** for all further analysis work\n",
    "3. **Leverage the examples** in cell 13 for advanced patterns\n",
    "4. **Build your custom analysis** on this solid foundation\n",
    "\n",
    "---\n",
    "\n",
    "**The notebook is now clean, organized, and ready for smooth analysis work!** 🎉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('merged_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge-allegro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
